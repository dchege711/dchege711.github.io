<!doctype html><html lang=en><head><title>Differential Privacy: A Primer for a Non-technical Audience | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="Differential Privacy: A Primer for a Non-technical Audience.  Kobbi Nissim; Thomas Steinke; Alexandra Wood; Micah Altman; Aaron Bembenek; Mark Bun; Marco Gaboardi; David R. O'Brien; Salil Vadhan. www.ftc.gov  . May 7, 2017.    What Does DP Guarantee? It is a question of whether a particular computation (not output) preserves privacy.
DP only guarantees that no information specific to an individual is revealed by the computation. DP doesn&rsquo;t protect against information that could be learned even with an individual opting out of a dataset, e...."><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/politics-and-governance/>Politics and Governance</a></li><li><a href=https://www.curiosities.dev/politics-and-governance/wws351-it-policy/>Information and Technology Policy [WWS 351]</a></li><li><a href=https://www.curiosities.dev/politics-and-governance/wws351-it-policy/07-privacy/>07. Privacy</a></li><li><a href=https://www.curiosities.dev/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/>Differential Privacy</a></li><li class=active><a href=https://www.curiosities.dev/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/2017-05-07-dp-primer-nontechnical/>Differential Privacy: A Primer for a Non-technical Audience</a></li></ul></nav><section><header><h1>Differential Privacy: A Primer for a Non-technical Audience</h1><p class=meta>Dated May 7, 2017;
last modified on Wed, 01 Jan 2025</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#what-does-dp-guarantee>What Does DP Guarantee?</a></li><li><a href=#the-privacy-loss-parameter>The Privacy Loss Parameter</a></li><li><a href=#relationship-to-information-privacy-laws>Relationship to Information Privacy Laws</a></li></ul></nav></aside><article id=main-article><blockquote><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=Nissim2017>Differential Privacy: A Primer for a Non-technical Audience<i>.</i></cite>
Kobbi Nissim; Thomas Steinke; Alexandra Wood; Micah Altman; Aaron Bembenek; Mark Bun; Marco Gaboardi; David R. O'Brien; Salil Vadhan.
<a href=https://www.ftc.gov/system/files/documents/public_comments/2017/11/00023-141742.pdf target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=www.ftc.gov" loading=lazy aria-hidden=true width=16 height=16>
<i>www.ftc.gov</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
May 7, 2017.
<i class="fas fa-fw fa-globe" aria-hidden=true></i></div></blockquote><h2 id=what-does-dp-guarantee>What Does DP Guarantee?</h2><p>It is a question of whether a particular computation (not output) preserves
privacy.</p><p>DP only guarantees that <em>no information specific to an individual</em> is revealed
by the computation. DP doesn&rsquo;t protect against information that could be learned
even with an individual opting out of a dataset, e.g. a study that shows smoking
increases cancer risk allows us to infer cancer risk for individuals that opted
out.</p><div class=comment-holder><div class=comment><p>Previously assumed that a few individuals could claim privacy guarantees.
\(\Deta\).</p></div></div><p>Suppose Alice reports that 202 out of 3,005 families at State University earn
$1m+. Later, Bob reports that 201 out of 3,004 families earn $1m+. Eve can infer
that the family missing in the Bob&rsquo;s report earns $1m+</p><p>Alice and Bob may claim that their reports preserved privacy, but the same can&rsquo;t
be said for the composition. DP would have added some noise, e.g. Alice reports
204 and Bob reports 199, and Eve wouldn&rsquo;t have gained information specific to
any family.</p><h2 id=the-privacy-loss-parameter>The Privacy Loss Parameter</h2><p>If were to mandate that the analysis give the same result without John&rsquo;s data,
and we have to guarantee the same for everyone, then the dataset is useless.
Instead, DP mandates that the outcome be approximately the same if an individual
opts out.</p><p>\( \epsilon \) measures the effect of each individual&rsquo;s information on the
output of the analysis. Smaller \(\epsilon\) results in smaller deviation
between real-world analysis and the opt-out scenario.</p><p>\( \epsilon = 0 \) makes the analysis mimic the opt-out scenario of all
individuals perfectly, but such an analysis wouldn&rsquo;t provide meaningful output.
As a rule of thumb, \( .001 \le \epsilon \le 1 \).</p><p>Say the non-DP analysis gives \(.013\). A DP analysis might give \(.012\)
the first time, \(0.0138\) the next time, etc.</p><p>Define event \( A \): the outcome of the analysis is between \(0.1\) and
\(0.2\).</p><p>Consider an analysis on some input data for which \( \mathbb{P}\{A\} = p \).
An analysis without John&rsquo;s data would make \( \mathbb{P}\{A\} = p' \). DP
guarantees that \( p \le (1 + \epsilon) \cdot p' \).</p><p>Say an insurer will deny John coverage if event \(A\) happens. If opting out
of the study puts \(\mathbb{P}\{A\} \le .05\), then participating increases
\(\mathbb{P}\{A\}\) to at most \(0.05 \cdot (1 + \epsilon)\).</p><div class=comment-holder><div class=comment><p>DP uses the opt-out scenario as the baseline. Some privacy will be lost, but not
too much. Subjects can then ask themselves, &ldquo;Can I tolerate the probability of
the dreaded consequence rising by at most \(p' \cdot \epsilon\)?</p></div></div><p>Because DP computations add enough noise to hide the contribution of any subset
of (roughly) \(1/\epsilon\) then the resulting statistics are not as accurate.
DP increases the \(n_{min}\) required to produce accurate statistics. If \(n
\le 1/\epsilon\), then DP most certainly doesn&rsquo;t produce any meaningful result.</p><p>A simplified analysis shows that if multiple analyses are done, then \(\epsilon
\le \sum \epsilon_{i}\). DP is currently the only framework that guarantees how
privacy risk accumulates over multiple analyses.</p><p>The difference between the real-world and the opt-out scenarios of a group of \(k\) individuals grows to at most \(k \cdot \epsilon\). Effectively, a meaningful privacy guarantee can be provided to groups of individuals of up to \(1/\epsilon\).</p><h2 id=relationship-to-information-privacy-laws>Relationship to Information Privacy Laws</h2><p>Personally Identifiable Information (PII) doesn&rsquo;t have a precise technical meaning. Some combination of attributes considered harmless by themselves may identify an individual, e.g. zipcode + gender + birthday.</p><p>De-identification involves turning PII into non-PII. Techniques include suppression of cells representing small groups, adding noise, swapping and generating synthetic data. But this doesn&rsquo;t protect individuals against linkage attacks.</p><p>Privacy laws often forbid PII, which excludes analyses such as finding the relationship between first names and lifetime earnings. DP makes such analysis possible because the input may have PII, but output doesn&rsquo;t reveal any PII.</p><p>Even in any linkage attack, the attacker cannot learn much more about an individual in a database than she could if that individual&rsquo;s information were not in the database.</p><p>Privacy laws often include consent and opt-out provisions. DP enables a more informed way to grant consent. And by definition, every individual in DP is granted an opt-out.</p><p>With DP, privacy laws can be well-defined, quantifiable, aware of composition, generalize to unknown attacks and apply universally.</p></article><div style=font-size:smaller><aside id=authors-holder style="margin:0 0 2%">Cited Authors:
<a href=/cited-authors/Altman-Micah>Altman, Micah</a>
<a href=/cited-authors/Bembenek-Aaron>Bembenek, Aaron</a>
<a href=/cited-authors/Bun-Mark>Bun, Mark</a>
<a href=/cited-authors/Gaboardi-Marco>Gaboardi, Marco</a>
<a href=/cited-authors/Nissim-Kobbi>Nissim, Kobbi</a>
<a href=/cited-authors/O%27Brien-David-R.>O'Brien, David R.</a>
<a href=/cited-authors/Steinke-Thomas>Steinke, Thomas</a>
<a href=/cited-authors/Vadhan-Salil>Vadhan, Salil</a>
<a href=/cited-authors/Wood-Alexandra>Wood, Alexandra</a></aside><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/www.ftc.gov style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=www.ftc.gov" loading=lazy aria-hidden=true width=16 height=16>
www.ftc.gov</a></aside></div></div><footer><a href=https://www.curiosities.dev/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/2010-narayanan-myths-and-fallacies-of-pii/>&#171; Myths and Fallacies of 'Personally Identifiable Information'</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>