<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Differential Privacy on c13u&#39;s Blog</title>
    <link>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/</link>
    <description>Recent content in Differential Privacy on c13u&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jan 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Motivation for Differential Privacy in Datasets</title>
      <link>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/01-big-data-and-privacy/</link>
      <pubDate>Mon, 25 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/01-big-data-and-privacy/</guid>
      <description>The goal is to make large datasets safe, i.e. no harm to you from having your data included.
Sanitize and Transfer Model Remove all PII, _e.g. name, SSN, mobile number, etc, before releasing the dataset.
However, the list of PII is not necessarily complete. Furthermore, combinations of seemingly non-PII data can be jointly identifying.
Using an auxilliary dataset is a common re-identification method, e.g. Narayanan and Shmatikov [2010]  combined IMDB ratings and comments to deanonymize a Netflix dataset.</description>
    </item>
    
    <item>
      <title>Myths and Fallacies of &#39;Personally Identifiable Information&#39;</title>
      <link>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/2010-narayanan-myths-and-fallacies-of-pii/</link>
      <pubDate>Tue, 01 Jun 2010 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/2010-narayanan-myths-and-fallacies-of-pii/</guid>
      <description>Myths and Fallacies of &amp;#39;Personally Identifiable Information&amp;#39;.  Narayanan, Arvind; Shmatikov, Vitaly. https://dl.acm.org/doi/fullHtml/10.1145/1743546.1743558  . Jun 1, 2010.   What is PII? From Breach Notification Laws: For example, California Senate Bill 1386: SSNs, driverâ€™s license numbers, financial accounts.
The list can never be exhaustive, e.g. email addresses and telephone numbers are not mentioned in Bill 1386.
Focuses on data that are commonly used for authenticating an individual. Ignores data that reveals some sensitive information about an individual</description>
    </item>
    
    <item>
      <title>Differential Privacy: A Primer for a Non-technical Audience</title>
      <link>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/2017-05-07-dp-primer-nontechnical/</link>
      <pubDate>Sun, 07 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/politics-and-governance/wws351-it-policy/07-privacy/differential-privacy/2017-05-07-dp-primer-nontechnical/</guid>
      <description>Differential Privacy: A Primer for a Non-technical Audience.  Kobbi Nissim; Thomas Steinke; Alexandra Wood; Micah Altman; Aaron Bembenek; Mark Bun; Marco Gaboardi; David R. O&amp;#39;Brien; Salil Vadhan. https://www.ftc.gov/system/files/documents/public_comments/2017/11/00023-141742.pdf  . May 7, 2017.   What Does DP Guarantee? It is a question of whether a particular computation (not output) preserves privacy.
DP only guarantees that no information specific to an individual is revealed by the computation. DP doesn&amp;rsquo;t protect against information that could be learned even with an individual opting out of a dataset, e.</description>
    </item>
    
  </channel>
</rss>
