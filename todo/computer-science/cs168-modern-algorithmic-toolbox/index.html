<!doctype html><html lang=en><head><title>[ToDo] CS 168: The Modern Algorithmic Toolbox | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="The Modern Algorithmic Toolbox (CS168).  Gregory Valiant. web.stanford.edu  . 2022.  Accessed Sep 10, 2022.  Modern Hashing  Consistent hashing. Property-preserving lossy compression. From majority elements to approximate heavy hitters. From bloom filters to the count-min sketch.  Data with Distances  Similarity Search. (Dis)similarity metrics: Jaccard, Euclidean, Lp. Efficient algorithm for finding similar elements in small/medium (i.e. \(< 20\)) dimensions using k-d trees. Curse of Dimensionality, kissing number...."><meta property="og:title" content="[ToDo] CS 168: The Modern Algorithmic Toolbox"><meta property="og:description" content="The Modern Algorithmic Toolbox (CS168).  Gregory Valiant. web.stanford.edu  . 2022.  Accessed Sep 10, 2022.  Modern Hashing  Consistent hashing. Property-preserving lossy compression. From majority elements to approximate heavy hitters. From bloom filters to the count-min sketch.  Data with Distances  Similarity Search. (Dis)similarity metrics: Jaccard, Euclidean, Lp. Efficient algorithm for finding similar elements in small/medium (i.e. \(< 20\)) dimensions using k-d trees. Curse of Dimensionality, kissing number...."><meta property="og:type" content="website"><meta property="og:url" content="https://www.curiosities.dev/todo/computer-science/cs168-modern-algorithmic-toolbox/"><meta property="og:site_name" content="curiosities.dev"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/todo/>To-Do List</a></li><li><a href=https://www.curiosities.dev/todo/computer-science/>[ToDo] Computer Science</a></li><li class=active><a href=https://www.curiosities.dev/todo/computer-science/cs168-modern-algorithmic-toolbox/>[ToDo] CS 168: The Modern Algorithmic Toolbox</a></li></ul></nav><section><header><h1>[ToDo] CS 168: The Modern Algorithmic Toolbox</h1><p class=meta>Dated Sep 10, 2022;
last modified on Sun, 11 Sep 2022</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#modern-hashing>Modern Hashing</a></li><li><a href=#data-with-distances>Data with Distances</a></li><li><a href=#generalization-and-regularization>Generalization and Regularization</a></li><li><a href=#linear-algebraic-techniques-understanding-principal-components-analysis-pca>Linear-Algebraic Techniques: Understanding Principal Components Analysis (PCA)</a></li><li><a href=#linear-algebraic-techniques-understanding-the-singular-value-decomposition-svd>Linear-Algebraic Techniques: Understanding the Singular Value Decomposition (SVD)</a></li><li><a href=#spectral-graph-theory>Spectral Graph theory</a></li><li><a href=#sampling-and-estimation>Sampling and Estimation</a></li><li><a href=#the-fourier-perspective-and-other-bases>The Fourier Perspective (and other bases)</a></li><li><a href=#sparse-vectormatrix-recovery-compressive-sensing-and-mathematical-programming>Sparse Vector/Matrix Recovery (Compressive Sensing) and Mathematical Programming</a></li><li><a href=#privacy-preserving-computation>Privacy Preserving Computation</a></li></ul></nav></aside><article id=main-article><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=CS168>The Modern Algorithmic Toolbox (CS168)<i>.</i></cite>
Gregory Valiant.
<a href=https://web.stanford.edu/class/cs168/index.html target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=web.stanford.edu" loading=lazy aria-hidden=true width=16 height=16>
<i>web.stanford.edu</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
2022.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Sep 10, 2022.</div><h2 id=modern-hashing>Modern Hashing</h2><ul><li>Consistent hashing.</li><li>Property-preserving lossy compression. From majority elements to approximate
heavy hitters. From bloom filters to the count-min sketch.</li></ul><h2 id=data-with-distances>Data with Distances</h2><ul><li>Similarity Search. (Dis)similarity metrics: Jaccard, Euclidean, Lp. Efficient
algorithm for finding similar elements in small/medium (i.e. \(&lt; 20\))
dimensions using k-d trees.</li><li>Curse of Dimensionality, kissing number. Distance-preserving compression.
Estimating Jaccard Similarity using MinHash. Euclidean distance preserving
dimensionality reduction (aka the Johnson-Lindenstrauss Transform)</li></ul><h2 id=generalization-and-regularization>Generalization and Regularization</h2><ul><li>Generalization (or, how much data is enough?). Learning an unknown function
from samples from an unknown distribution. Training error vs. test error. PAC
guarantees for linear classifiers. Empirical risk minimization.</li><li>Regularization. The polynomial embedding and random projection, L2
regularization, and L1 regularization as a computationally tractable surrogate
for L0 regularization.</li></ul><h2 id=linear-algebraic-techniques-understanding-principal-components-analysis-pca>Linear-Algebraic Techniques: Understanding Principal Components Analysis (PCA)</h2><ul><li>Understanding PCA. Minimizing squared distances equals minimizing variance.
Use case for data visualization and data compression. Failure modes for PCA.</li><li>How PCA works. Maximizing variance as finding the &ldquo;direction of maximum
stretch&rdquo; of the covariance matrix. The simple geometry of &ldquo;diagonals in
disguise.&rdquo; The power iteration algorithm.</li></ul><h2 id=linear-algebraic-techniques-understanding-the-singular-value-decomposition-svd>Linear-Algebraic Techniques: Understanding the Singular Value Decomposition (SVD)</h2><ul><li>Low-rank matrix approximations. SVD; applications to matrix compression,
de-noising, and matrix completion (i.e. recovering missing entries).</li><li>Tensor methods. Differences between matrices and tensors, the uniqueness of
low-rank tensor factorizations, and Jenrich&rsquo;s algorithm.</li></ul><h2 id=spectral-graph-theory>Spectral Graph theory</h2><ul><li>Graphs as matrices and the Laplacian of a graph, Interpretations of the
largest and smallest eigenvectors/eigenvalues of the Laplacian. Spectral
embeddings, and an overview of applications (e.g. graph coloring, spectral
clustering).</li><li>Interpretations of the second eigenvalue (via conductance and isoperimetric
number), and the connections with the speed at which random walks/diffusions
converge.</li></ul><h2 id=sampling-and-estimation>Sampling and Estimation</h2><ul><li>Reservoir sampling (how to select a random sample from a data stream). Basic
probability tools: Markov&rsquo;s inequality and Chebyshev&rsquo;s inequality. Importance
Sampling (how to make inferences about one distribution based on samples from
a different distribution). Description of the Good-Turing estimate of
missing/unseen mass.</li><li>Markov Chains, stationary distributions. Markov Chain Monte Carlo (MCMC) as
approaches to solving hard problems by sampling from carefully crafted
distributions.</li></ul><h2 id=the-fourier-perspective-and-other-bases>The Fourier Perspective (and other bases)</h2><ul><li>Fourier methods, part 1.</li><li>Fourier methods, part 2 (emphasis on convolutions).</li></ul><h2 id=sparse-vectormatrix-recovery-compressive-sensing-and-mathematical-programming>Sparse Vector/Matrix Recovery (Compressive Sensing) and Mathematical Programming</h2><ul><li>Compressive sensing.</li><li>Linear and convex programming. Matrix completion.</li></ul><h2 id=privacy-preserving-computation>Privacy Preserving Computation</h2><ul><li>Differential Privacy in data analysis and machine learning</li></ul></article><div style=font-size:smaller><aside id=authors-holder style="margin:0 0 2%">Cited Authors:
<a href=/cited-authors/Valiant-Gregory>Valiant, Gregory</a></aside><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/web.stanford.edu style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=web.stanford.edu" loading=lazy aria-hidden=true width=16 height=16>
web.stanford.edu</a></aside></div></div><footer><a href=https://www.curiosities.dev/todo/computer-science/concepts-techniques-and-models-of-computer-programming/>&#171; [ToDo] Concepts, Techniques, and Models of Computer Programming</a>
<a href=https://www.curiosities.dev/todo/computer-science/the-missing-semester-of-your-cs-education/>[ToDo] The Missing Semester of Your CS Education &#187;</a></footer></section></div><footer><a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>