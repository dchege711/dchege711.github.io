<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Liang, Percy on Chege's Blog</title><link>https://www.curiosities.dev/cited-authors/Liang-Percy/</link><description>Recent content in Liang, Percy on Chege's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 14 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/cited-authors/Liang-Percy/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to LLMs</title><link>https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/</guid><description>What is a Language Model? A language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \(\mathcal{V}\) of a set of tokens, then a language model \(p\) assigns each sequence of tokens \(x_1, &amp;hellip;, x_L \in \mathcal{V} \) a probability.
To assign meaningful probabilities to all sequences requires syntactic knowledge and world knowledge. Given \( \mathcal{V} = \{ \text{ate}, \text{ball}, \text{cheese}, \text{mouse}, \text{the} \} \):</description></item></channel></rss>