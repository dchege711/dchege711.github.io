<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ré, Christopher on Chege's Blog</title><link>https://www.curiosities.dev/cited-authors/R%C3%A9-Christopher/</link><description>Recent content in Ré, Christopher on Chege's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 30 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/cited-authors/R%C3%A9-Christopher/index.xml" rel="self" type="application/rss+xml"/><item><title>Given Language Models, Why Learn About Large Language Models?</title><link>https://www.curiosities.dev/computer-science/large-language-models/why-learn-about-llms/</link><pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/why-learn-about-llms/</guid><description>This part of seems pertinent to respond to &amp;ldquo;LLMs are just (auto-complete; Markov chains; [insert pre-existing LM-adjacent tech]) on steroids&amp;rdquo;.
Scale LLMs are massive. From 2018 - 2022, model sizes have increased 5000x. OpenAI&amp;rsquo;s GPT model from June 2018 had 110M parameters; GPT-3 from May 2020 had 175B parameters. LLM providers no longer seem to advertise their parameter counts; GPT-4 was leaked to have 1.8T parameters.</description></item><item><title>Introduction to LLMs</title><link>https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/</guid><description>What is a Language Model? A language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \(\mathcal{V}\) of a set of tokens, then a language model \(p\) assigns each sequence of tokens \(x_1, &amp;hellip;, x_L \in \mathcal{V} \) a probability.
To assign meaningful probabilities to all sequences requires syntactic knowledge and world knowledge. Given \( \mathcal{V} = \{ \text{ate}, \text{ball}, \text{cheese}, \text{mouse}, \text{the} \} \):</description></item><item><title>[ToDo] CS 324: Large Language Models</title><link>https://www.curiosities.dev/todo/ai-and-machine-learning/cs324-large-language-models/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/todo/ai-and-machine-learning/cs324-large-language-models/</guid><description>Lectures | CS324. Percy Liang; Tatsunori Hashimoto; Christopher Ré. stanford-cs324.github.io . 2022. Accessed Dec 14, 2023. ✅ Introduction: What is an LM? A brief history . Why does CS 324 exist? Capabilities: What are the capabilities of GPT-3? Harms I &amp;amp; II: Performance disparities, social biases and stereotypes, toxicity, and misinformation. Data: Data behind LLMs; documentation of datasets; data ecosystems.</description></item></channel></rss>