<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Gebru, Timnit on Chege's Blog</title><link>https://www.curiosities.dev/cited-authors/Gebru-Timnit/</link><description>Recent content in Gebru, Timnit on Chege's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 04 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/cited-authors/Gebru-Timnit/index.xml" rel="self" type="application/rss+xml"/><item><title>Journal Reviews on Fairness</title><link>https://www.curiosities.dev/computer-science/bias-and-fairness/2021-10-04-journal-reviews-on-fairness/</link><pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/bias-and-fairness/2021-10-04-journal-reviews-on-fairness/</guid><description>Meta ðŸ“‘ Instead of changing the data or learners in multiple ways and then see if fairness improves, postulate that the root causes of bias are the prior decisions that generated the training data. These affect (a) what data was selected, and (b) the labels assigned to the examples. They propose the \(\text{Fair-SMOTE}\) (Fair Synthetic Minority Over Sampling Technique) algorithm which (1) removes biased labels (via situation testing: if the model&amp;rsquo;s prediction for a data point changes once all of the data points' protected attributes are flipped, then that label is biased and the data point is discarded), and (2) rebalances internal distributions such that based on a protected attribute, examples are equal in both positive and negative classes.</description></item><item><title>On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</title><link>https://www.curiosities.dev/computer-science/bias-and-fairness/2021-03-03-on-the-dangers-of-stochastic-parrots/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/bias-and-fairness/2021-03-03-on-the-dangers-of-stochastic-parrots/</guid><description>The paper is written in a period when NLP practitioners are producing bigger (# of parameters; size of training data) language models (LMs), and pushing the top scores on benchmarks.
Environmental Risks Large LMs consume a lot of resources, e.g. training a single BERT base model on GPUs was estimated to use as much energy as a trans-American flight.
Marginalized communities are doubly punished. They are least likely to benefit from LMs, e.</description></item></channel></rss>