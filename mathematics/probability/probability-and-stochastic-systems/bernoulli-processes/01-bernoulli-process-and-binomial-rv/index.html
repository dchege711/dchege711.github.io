<!DOCTYPE html>
<html>

    <head>
        <title>
             
                Bernoulli Processes and the Binomial Random Variable | c13u
            
        </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" type="text/css" href="/css/main.css" />
        <link rel="stylesheet" type="text/css" href="/css/all_font_awesome_v5.9.min.css" />
        
        <link rel="shortcut icon" href="/img/favicon_io/favicon.ico">
        <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon_io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon_io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon_io/favicon-16x16.png">

        <link rel="stylesheet" href="/css/vs.css">
        <script type="text/javascript" src="/js/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script type="text/javascript" src="/js/d3/d3.min.js"></script>
        <script type="text/javascript" src="/js/PlotUtils.js"></script>
        <script type="text/javascript" src="/js/OrganizeCitations.js"></script>

        
        
    </head>

    <body>

        <div class="container" id="main_div">

            
            <form action="/search" method="get" id="globalSearchForm">
                <input type="text" id="q" name="q">
                <input type="submit" id="submitButton" value="Search">
            </form>
            
            
            
            <nav aria-label="Breadcrumb" class="breadcrumb">
    <ul>
        











<li>
  <a href="https://www.c13u.com/">Home</a>
</li>


<li>
  <a href="https://www.c13u.com/mathematics/">Mathematics</a>
</li>


<li>
  <a href="https://www.c13u.com/mathematics/probability/">Probability</a>
</li>


<li>
  <a href="https://www.c13u.com/mathematics/probability/probability-and-stochastic-systems/">Probability and Stochastic Systems [ORF 309]</a>
</li>


<li>
  <a href="https://www.c13u.com/mathematics/probability/probability-and-stochastic-systems/bernoulli-processes/">Bernoulli Processes</a>
</li>


<li class="active">
  <a href="https://www.c13u.com/mathematics/probability/probability-and-stochastic-systems/bernoulli-processes/01-bernoulli-process-and-binomial-rv/">Bernoulli Processes and the Binomial Random Variable</a>
</li>

    </ul>
</nav>


            
            
<section>
    <header>
    <h1> Bernoulli Processes and the Binomial Random Variable</h1>
    <p class="meta">
        
        Dated Oct 2, 2017; 
        
        last modified on Sat, 25 Apr 2020
        
    </p>
    </header>

    <div id="toc-then-article">
        <aside id="toc">
            <nav id="TableOfContents">
  <ul>
    <li><a href="#bernoulli-process">Bernoulli Process</a></li>
    <li><a href="#sample-problems">Sample Problems</a>
      <ul>
        <li><a href="#probability-of-specific-sequences">Probability of Specific Sequences</a></li>
        <li><a href="#probability-of-general-sequences">Probability of General Sequences</a></li>
      </ul>
    </li>
    <li><a href="#the-binomial-random-variable">The Binomial Random Variable</a>
      <ul>
        <li><a href="#sanity-check-do-the-probabilities-sum-to-1">Sanity Check: Do the probabilities sum to 1?</a></li>
        <li><a href="#the-expectation-of-the-binomial-random-variable">The Expectation of the Binomial Random Variable</a>
          <ul>
            <li><a href="#nave-solution">Naïve Solution</a></li>
            <li><a href="#clever-solution">Clever Solution</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
        </aside>

        <article id="main-article">
            <h2 id="bernoulli-process">Bernoulli Process</h2>
<p>A <strong>Bernoulli Process</strong> is a sequence of independent \({0, 1}\) - valued random variables \(X_1, X_2, X_3, &hellip;\), e.g. \(0, 0, 1, 0, 1, 1\)</p>



<div class="comment-holder">
    <div class="comment"><p>A Bernoulli Process does not mandate that the probability distributions of the \(X_i\) be identical. That is up to the model that we choose. For instance, the Binomial Random Variable assumes \(\mathbb{P}\{X_i = 1\} = p \ \ \forall i\)</p>
</div>
</div>

<p>Suppose you flip a coin repeatedly, and record \(0\) for tails and \(1\) for heads.</p>
<ul>
<li>How many flips until the first \(1\)?</li>
<li>Is it possible to get observe an infinite sequence of \(0\)&lsquo;s?</li>
<li>What is the probability of getting 51 ones among 100 coin tosses?</li>
</ul>
<h2 id="sample-problems">Sample Problems</h2>
<h3 id="probability-of-specific-sequences">Probability of Specific Sequences</h3>
<blockquote>
<p>Suppose you have a coin in which \(\mathbb{P}(H) = \frac{2}{3} \). If you toss it 5 times, what is the probability of getting three \(1\)&lsquo;s and two \(0\)&lsquo;s? ()</p>
</blockquote>
<p>Our favorable outcomes are: \((1,1,1,0,0), (0,1,1,1,0), &hellip;, (0,1,0,1,1) \therefore \) 10 favorable outcomes.</p>



<div class="comment-holder">
    <div class="comment"><p>Counting can be tricky. In how many ways can we put 3 ones in 5 slots? \( {5 \choose 3} = \frac{5!}{2! \times 3!} = 10 \)</p>
</div>
</div>

<p>Each of the favorable outcomes has equal probability of occurring, e.g.</p>
<p>$$ \mathbb{P}\{(1,1,1,0,0)\} = \left( \frac{2}{3} \right)^3 \cdot \left( \frac{1}{3} \right)^2 $$</p>
<p>Because all of these outcomes are disjoint, we can sum them up to get the final probability as:</p>
<p>$$ \left( \frac{2}{3} \right)^3 \cdot \left( \frac{1}{3} \right)^2 \cdot 10 $$</p>
<h3 id="probability-of-general-sequences">Probability of General Sequences</h3>
<blockquote>
<p>If \(\mathbb{P}\{H\} = p\), what is the probability of \(k\) heads in \(n\) experiments?</p>
</blockquote>
<p>Where \(x\) is the number of sequences of length \(n\) with \(k\) ones and \(n-k\) zeros, then the answer is:</p>
<p>$$ = p^k \cdot (1 - p)^{n-k} \cdot x $$</p>
<p>Let's find \(x\)&hellip;</p>
<p>$$ x = \frac{n (n-1) &hellip; (n - k + 1)}{k (k-1) &hellip; (1)} = \frac{n!}{k! (n-k)!} = { n \choose k } = C_{n}^{k} $$</p>



<div class="comment-holder">
    <div class="comment"><p>For the numerator, we're trying to place \(k\) ones in \(n\) slots. The first one can be placed in any of the \(n\) slots, the second one can be placed in any of the remaining \(n - 1\) slots, and so forth, until the last one which can be placed into any of the remaining \( n - (k - 1) = n - k + 1 \) slots.</p>
<p>For the denominator, because all the ones are identical, we don't care about the ordering of the individual ones. We therefore divide by the number of possible orderings of \(k\) items.</p>
</div>
</div>

<p>And therefore, the final answer is:</p>
<p>$$ { n \choose k } \cdot p^k \cdot (1 - p)^{n-k} $$</p>
<h2 id="the-binomial-random-variable">The Binomial Random Variable</h2>
<p>The above result is important enough to have a name.</p>
<p>\(X\) is a binomial random variable if it takes the values \(0, 1, 2, &hellip;, n\) and \(\mathbb{P}\{X = k\} = { n \choose k } \cdot p^k \cdot (1 - p)^{n-k} \)</p>
<h3 id="sanity-check-do-the-probabilities-sum-to-1">Sanity Check: Do the probabilities sum to 1?</h3>
<p>$$ \sum_{k=0}^{n} \mathbb{P}\{X = k\} = \sum_{k=0}^{n} { n \choose k } p^k (1 - p)^{n-k} = \left( p + (1 - p) \right)^n = 1 $$</p>



<div class="comment-holder">
    <div class="comment"><p>I totally didn't understand how we got to \(\left( p + (1 - p) \right)^n\). In my notes, I simply noted &ldquo;Calculus Theorem&rdquo; and that was it.</p>
<p>ლ(ಠ_ಠ ლ)</p>
<p>A couple of web searches led me to <a href="https://en.wikipedia.org/wiki/Binomial_theorem#Theorem_statement">The Binomial Theorem (en.wikipedia.org)</a>, which states:</p>
<p>$$ (x + y)^n = {n \choose 0} x^n y^0 + {n \choose 1} x^{n-1} y^1 + {n \choose 2} x^{n-1} y^2 + &hellip; + {n \choose n - 1} x^1 y^{n-1} + {n \choose n} x^0 y^n $$</p>
<p>where \(n \ge 0\) is an integer, and each \( {n \choose k} \) is a positive integer known as a binomial coefficient. \( {n \choose 0} \) is defined as equal to 1.</p>
<p>So much for &ldquo;recall from freshman calculus that&hellip;&rdquo;</p>
</div>
</div>

<h3 id="the-expectation-of-the-binomial-random-variable">The Expectation of the Binomial Random Variable</h3>
<h4 id="nave-solution">Naïve Solution</h4>
<p>Using the definition of expectation:</p>
<p>$$ \mathbb{E}[X] = \sum_{k=0}^{n} \mathbb{P\{X = k\}} k = \sum_{k=0}^{n} { n \choose k } p^k (1 - p)^{n-k} \cdot k $$</p>



<div class="comment-holder">
    <div class="comment"><p>Because \( { n \choose k } = \frac{n!}{k! (n-k)!} \), we can simplify further.</p>
<p>Also, note that when \(k = 0\), the summation term is \(0\), and we can therefore exclude it from the summation.</p>
</div>
</div>

<p>$$ = \sum_{k=1}^{n} \frac{n!}{(k-1)! (n-k)!} p^k (1-p)^{n-k} $$</p>



<div class="comment-holder">
    <div class="comment"><p>Let's massage the summation such that we can invoke the binomial formula&hellip;</p>
</div>
</div>

<p>$$ = n \cdot \sum_{k=1}^{n} \frac{(n - 1)!}{(k-1)! (n-k)!} p^k (1-p)^{n-k} $$
$$ = np \cdot \sum_{k=1}^{n} \frac{(n - 1)!}{(k-1)! (n-k)!} p^{k-1} (1-p)^{n-k} $$</p>



<div class="comment-holder">
    <div class="comment"><p>We're gonna do what is called a pro gamer move. Change of variables. Let \(i = k - 1\), equivalently \( k = i + 1\)</p>
</div>
</div>

<p>$$ = np \cdot \sum_{i=0}^{n-1} \frac{(n - 1)!}{i! (n - i - 1)!} p^i (1-p)^{n-i-1} $$
$$ = np \cdot \sum_{i=0}^{n-1} \frac{(n - 1)!}{i! ((n - 1) - i)!} p^i (1-p)^{((n-1)-i)} $$
$$ = np \sum_{i=0}^{n-1} {n - 1 \choose i} p^i (1-p)^{((n-1) - i)} $$</p>



<div class="comment-holder">
    <div class="comment"><p>And now applying the binomial formula&hellip;</p>
</div>
</div>

<p>$$ = np (p + (1 - p))^{n-1} = np $$</p>



<div class="comment-holder">
    <div class="comment"><p>Tbh, I didn't see that coming. Too many tricks and eurekas.</p>
</div>
</div>

<h4 id="clever-solution">Clever Solution</h4>
<p>Where \(X_i = 1\) if \(i\)-th toss is heads, and \(X_i = 0\) otherwise:</p>
<p>$$ \mathbb{E}[X] = \mathbb{E}[X_1 + X_2 + &hellip; + X_n] $$</p>



<div class="comment-holder">
    <div class="comment"><p>Not quite certain why this is so. Maybe it's because the \(X_i\)&lsquo;s are disjoint?</p>
</div>
</div>

<p>$$ = \mathbb{E}[X_1] + \mathbb{E}[X_2] + &hellip;. + \mathbb{E}[X_n] $$</p>



<div class="comment-holder">
    <div class="comment"><p><strong>Linearity of Expectation</strong> is the property that for any random variables \(X\) and \(Y\), and a constant \(a\),</p>
<p>$$ \mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y] $$
$$ \mathbb{E}[aX] = a \mathbb{E}[X] $$</p>
<p>whenever the right-hand side is well defined (finite?). It doesn't matter whether \(X\) and \(Y\) are independent. <a href="https://en.wikipedia.org/wiki/Expected_value#Basic_properties">Source: Expected value - Wikipedia</a></p>
</div>
</div>

<p>$$ = n \left( p \cdot 1 + (1-p) \cdot 0 \right) = np $$</p>

        </article>

        
    </div>
    <footer>
        
        
        
            
        

        
        

    </footer>
</section>


        </div>

        <footer>
            <a href="mailto:d.chege711@gmail.com">Email</a>
            
            <a href="/about">About</a>
            <a href="/search">Search</a>
        </footer>

    </body>

</html>
