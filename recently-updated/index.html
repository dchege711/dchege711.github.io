<!DOCTYPE html>
<html>

    <head>
        <title>
             
                Recently Updated Pages | c13u
            
        </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" type="text/css" href="/css/main.css" />
        <link rel="stylesheet" type="text/css" href="/css/all_font_awesome_v5.9.min.css" />
        
        <link rel="shortcut icon" href="/img/favicon_io/favicon.ico">
        <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon_io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon_io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon_io/favicon-16x16.png">

        <link rel="stylesheet" href="/css/vs.css">
        <script type="text/javascript" src="/js/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script type="text/javascript" src="/js/d3/d3.min.js"></script>
        <script type="text/javascript" src="/js/PlotUtils.js"></script>
        <script type="text/javascript" src="/js/OrganizeCitations.js"></script>
        <script type="text/javascript" src="/js/HighlightAnchor.js"></script>

        
        
    </head>

    <body>

        <div class="container" id="main_div">

            
            <form action="/search" method="get" id="globalSearchForm">
                <input type="text" id="q" name="q">
                <input type="submit" id="submitButton" value="Search">
            </form>
            
            
            
            <nav aria-label="Breadcrumb" class="breadcrumb">
    <ul>
        



<li>
  <a href="https://www.c13u.com/">Home</a>
</li>


<li class="active">
  <a href="https://www.c13u.com/recently-updated/">Recently Updated Pages</a>
</li>

    </ul>
</nav>


            
            
<p>A list of the 15 most recently updated pages.</p>


<table border="0">




    
        
    <tr class="no-decoration">
        <td class="no-decoration">Jan 22, 2019</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/cos432-information-security/14-privacy/01-privacy-in-cs-and-in-the-law/">Privacy in CS and in the Law</a>
            
                
                    <span class="meta">
                    4 min; updated Oct 7, 2020
                    </span>
                    <p class="meta">
                        Incomplete List of Information Privacy Properties  Control/consent, e.g. Cambridge Analytica exfiltrating FB users&rsquo; data   Discussion on Cambridge Analytica  
    Anonymity, e.g. Snowden leaking NSA docs without revealing his identity
  Limits on data collection, e.g. laws restricting government surveillance
  Limits on data use, e.g. US Genetic Information Nondiscrimination Act of 2008
  Under GINA, health insurers must not use genetic information of the clients (or clients family) to inform their policy....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 7, 2020</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/business/optimism_and_deception/2020-10-07-environment-and-dollars/">Environmentalism and Capitalism</a>
            
                
                    <span class="meta">
                    1 min; updated Oct 7, 2020
                    </span>
                    <p class="meta">
                        Recycling was a lie to sell more plastic [] Over the last 70 years, less than 10% of plastic waste has been recycled - it&rsquo;s uneconomical. Recycling shifts attention from the environmental impact of plastics and their overproduction to the consumer&rsquo;s willingness to recycle. Reducing and reusing is a much better environmental strategy.
References  Recycling was a lie — a big lie — to sell more plastic, industry experts say....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Aug 9, 2018</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/08-curse-of-dimensionality-and-feature-selection/">The Curse of Dimensionality and Feature Selection</a>
            
                
                    <span class="meta">
                    3 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        The Curse of Dimensionality The predictive power of an induced model is based either on:
  Partitioning the feature space into regions based on clusters of training instances and assigning a query located in region \(X\) the target value of the training instances in that cluster.
  Interpolating a target value from the target values of individual training instances that are near the query in the feature space....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/07-other-measures-of-similarity/">Other Measures of Similarity in NN</a>
            
                
                    <span class="meta">
                    2 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        Note: similarity indexes may not specify the 4 NIST specifications of a similarity metric .
Consider these two instances:
    \(f_1\) \(f_2\) \(f_3\) \(f_4\) \(f_5\)     d 0 1 1 0 1   q 0 0 1 1 1    \(CP(q, d)\), the number of the co-presences of binary features, is \(2\) because of \(f_3\) and \(f_4\).
\(CA(q, d)\), the number of co-absences, is \(1\) because of \(f_1\)....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/06-predicting-continuous-targets/">Predicting Continuous Targets Using NN</a>
            
                
                    <span class="meta">
                    1 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        Return the Average Value One possible solution is to return the average value in the neighborhood, i.e.
$$ \mathbb{M}_{k}(q) = \frac{1}{k} \sum_{i=1}^{k} t_i $$
We can improve this by using weighted \(k-NN\):
$$ \mathbb{M}_{k}(q) = \frac{ \sum_{i=1}^{k} \left( \frac{1}{dist(q, d_i)^2} \cdot t_i \right) }{ \sum_{i=1}^{k} \frac{1}{dist(q, d_i)^2} } $$
The formula looks new. However, if \(x_1\) is weighted by \(w_1\) and \(x_2\) by \(w_2\), then the weighted average is:
$$ \frac{w_1 x_1 + w_2 x_2 }{w_1 + w_2} $$...
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/05-the-case-for-range-normalization/">The Case for Range Normalization</a>
            
                
                    <span class="meta">
                    1 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        When you have features taking different range if values, you may have odd predictions.
For example, if \(f_1 \in [0, 100]\) and \(f_2 \in [0, 1]\), \(f_1\) will always be penalized more than \(f_2\) when computing the distance.
  To mitigate this, normalize the feature&rsquo;s ranges to \([r_{low}, r_{high}]\):
$$ a&rsquo;_i = \frac{a_i - a_{min}}{ a_{max} - a_{min}} \cdot (r_{high} - r_{low}) + r_{low} $$
Typically, the range is normalized to \([r_{low}, r_{high}] = [0, 1]\), so range normalization simplifies to:...
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/04-handling-noisy-data/">Handling Noisy Data in Nearest Neighbors</a>
            
                
                    <span class="meta">
                    1 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        Majority Voting The \(k\) nearest neighbors model predicts the target level from the majority vote from the set of the \(k\) nearest neighbors to the query \(q\).
Where \(\delta\) is an indicator function such that \(\delta(t_i, l) = 1 \iff t_i = l\):
$$ \mathbb{M}_{k} (q) = argmax_{l \in levels(t)} \left( \sum_{i=1}^{k} \delta(t_i, l) \right) $$
For categorical features, \(k\) should be odd to avoid ties.
This doesn&rsquo;t read right. If there are 3 possible categories, \(k = 3\) can result in a tie....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 10, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/03-caveats-on-similarity-learning/">Caveats on Similarity Learning</a>
            
                
                    <span class="meta">
                    1 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        Similarity-based learning is intuitive and gives people confidence in the model.
There is an inductive bias that instances that have similar descriptive features belong to the same class.
Remarkably so. When I think of classifying things, my mind immediately goes to NN.
  Similarity learning has a stationary assumption, i.e. the joint PDF of the data doesn&rsquo;t change (new classifications do not come up). This assumption is shared by supervised ML....
                    </p>
                
            
        </td>
    </tr>

    

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Jan 2, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/writing/the-new-strategy-of-style/02-subject-and-thesis/">02. The Subject</a>
            
                
                    <span class="meta">
                    3 min; updated Sep 6, 2020
                    </span>
                    <p class="meta">
                        The Subject The number of words aren&rsquo;t intuitive to me. Adults tend to read non-fiction at 238 words per minute. [] So a good test for how appropriate a piece of writing is, &ldquo;Can the writer cover X in N minutes?&rdquo;
Hugo uses 213 WPM, while Medium uses 265 WPM, suggesting Medium has a bias for skimming over close reading. []
I wonder if a browser extension that rates the appropriateness of articles given their subject and their word count is a feasible/desirable thing....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Apr 18, 2020</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/business/2020-04-18-econ-101-real-world-edition/">Economics in the Real World</a>
            
                
                    <span class="meta">
                    3 min; updated Aug 9, 2020
                    </span>
                    <p class="meta">
                        Protectionism vs. Globalization Prestige Ameritech is America&rsquo;s #1 maker of surgical masks. In outbreaks, demand peaks, but when it&rsquo;s all over, hospitals go back to exclusively buying cheaper masks from China. Reduced demand makes Ameritech lay off workers. Ameritech is pushing for a federal contract to stabilize demand over time. []
India&rsquo;s Foreign Exchange Act of 1973 gave Coca-Cola an ultimatum: hand over 60% of the local subsidiary to Indian partners and the syrup recipe....
                    </p>
                
            
        </td>
    </tr>

    

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Sep 29, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/mathematics/probability/probability-and-stochastic-systems/01-sample-space-and-probability/05-the-bayes-formula/">The Bayes Formula</a>
            
                
                    <span class="meta">
                    1 min; updated Jul 28, 2020
                    </span>
                    <p class="meta">
                        The Formula By definition&hellip;
$$ \mathbb{P}(A) = \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^{c}) $$
From conditional probability &hellip;
$$ \mathbb{P}(A) = \mathbb{P}(A|B) \ \mathbb{P}(B) + \mathbb{P}(A|B^c) \ \mathbb{P}(B^c) $$
Therefore
$$ \mathbb{P}(B|A) = \frac{ \mathbb{P}(B \cap A) }{ \mathbb{P}(A) } $$ $$ = \frac{ \mathbb{P}(A|B) \mathbb{P}(B) }{ \mathbb{P}(A|B) \ \mathbb{P}(B) + \mathbb{P}(A|B^c) \ \mathbb{P}(B^c) } $$
Switching the roles of the events is convenient because in many problems, one of the conditional probabilities is easier to calculate....
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Oct 9, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/computer-science/networks-friends-money-bytes/data-in-networks/amazon-rankings/bayesian-ranking/">Bayesian Rating</a>
            
                
                    <span class="meta">
                    1 min; updated Jul 28, 2020
                    </span>
                    <p class="meta">
                        Allows us to weight by review population size.
  Let \(n_i\) be the number of reviews that item \(i\) gets, and let \(r_i\) be the naive average rating of item \(i\)
  Let \(N\) be the total number of reviews across brands, i.e. \(N = \sum_{i} n_i \)
  Let \(R\) be the average rating over all items across brands, i.e. \(R = \frac{1}{N} \sum_{i} n_i r_i \)...
                    </p>
                
            
        </td>
    </tr>

    

    
        
    <tr class="no-decoration">
        <td class="no-decoration">Jul 26, 2020</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.c13u.com/mathematics/statistics/statistics-for-applications/08-bayesian-statistics/01-frequentists-vs-bayesian/">Frequentist Approach vs. Bayesian Approach</a>
            
                
                    <span class="meta">
                    3 min; updated Jul 28, 2020
                    </span>
                    <p class="meta">
                        The Frequentist Approach  Observe data. Assume the data were generated randomly, e.g. by nature, by designing a survey, etc. Make assumptions on the generating process, e.g. i.i.d., Gaussian, etc. Associate the generating process to some object of interest, e.g. a parameter, a density, etc. Assuming that the object is unknown but fixed, try to find it, e.g. estimate it, test a hypothesis about it, etc.  The Bayesian Approach  Observe data....
                    </p>
                
            
        </td>
    </tr>

    


</table>



        </div>

        <footer>
            <a href="mailto:d.chege711@gmail.com">Email</a>
            
            <a href="/about">About</a>
            <a href="/search">Search</a>
        </footer>

    </body>

</html>
