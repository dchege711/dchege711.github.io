<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Similarity Based Learning on c13u&#39;s Blog</title>
    <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/</link>
    <description>Recent content in Similarity Based Learning on c13u&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Oct 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Caveats on Similarity Learning</title>
      <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/03-caveats-on-similarity-learning/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/03-caveats-on-similarity-learning/</guid>
      <description>Similarity-based learning is intuitive and gives people confidence in the model.
There is an inductive bias that instances that have similar descriptive features belong to the same class.
Remarkably so. When I think of classifying things, my mind immediately goes to NN.
  Similarity learning has a stationary assumption, i.e. the joint PDF of the data doesn&amp;rsquo;t change (new classifications do not come up). This assumption is shared by supervised ML.</description>
    </item>
    
    <item>
      <title>Handling Noisy Data in Nearest Neighbors</title>
      <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/04-handling-noisy-data/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/04-handling-noisy-data/</guid>
      <description>Majority Voting The \(k\) nearest neighbors model predicts the target level from the majority vote from the set of the \(k\) nearest neighbors to the query \(q\).
Where \(\delta\) is an indicator function such that \(\delta(t_i, l) = 1 \iff t_i = l\):
$$ \mathbb{M}_{k} (q) = argmax_{l \in levels(t)} \left( \sum_{i=1}^{k} \delta(t_i, l) \right) $$
For categorical features, \(k\) should be odd to avoid ties.
This doesn&amp;rsquo;t read right. If there are 3 possible categories, \(k = 3\) can result in a tie.</description>
    </item>
    
    <item>
      <title>The Case for Range Normalization</title>
      <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/05-the-case-for-range-normalization/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/05-the-case-for-range-normalization/</guid>
      <description>When you have features taking different range if values, you may have odd predictions.
For example, if \(f_1 \in [0, 100]\) and \(f_2 \in [0, 1]\), \(f_1\) will always be penalized more than \(f_2\) when computing the distance.
  To mitigate this, normalize the feature&amp;rsquo;s ranges to \([r_{low}, r_{high}]\):
$$ a&amp;rsquo;_i = \frac{a_i - a_{min}}{ a_{max} - a_{min}} \cdot (r_{high} - r_{low}) + r_{low} $$
Typically, the range is normalized to \([r_{low}, r_{high}] = [0, 1]\), so range normalization simplifies to:</description>
    </item>
    
    <item>
      <title>Predicting Continuous Targets Using NN</title>
      <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/06-predicting-continuous-targets/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/06-predicting-continuous-targets/</guid>
      <description>Return the Average Value One possible solution is to return the average value in the neighborhood, i.e.
$$ \mathbb{M}_{k}(q) = \frac{1}{k} \sum_{i=1}^{k} t_i $$
We can improve this by using weighted \(k-NN\):
$$ \mathbb{M}_{k}(q) = \frac{ \sum_{i=1}^{k} \left( \frac{1}{dist(q, d_i)^2} \cdot t_i \right) }{ \sum_{i=1}^{k} \frac{1}{dist(q, d_i)^2} } $$
The formula looks new. However, if \(x_1\) is weighted by \(w_1\) and \(x_2\) by \(w_2\), then the weighted average is:
$$ \frac{w_1 x_1 + w_2 x_2 }{w_1 + w_2} $$</description>
    </item>
    
    <item>
      <title>Other Measures of Similarity in NN</title>
      <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/07-other-measures-of-similarity/</link>
      <pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/07-other-measures-of-similarity/</guid>
      <description>Note: similarity indexes may not specify the 4 NIST specifications of a similarity metric .
Consider these two instances:
    \(f_1\) \(f_2\) \(f_3\) \(f_4\) \(f_5\)     d 0 1 1 0 1   q 0 0 1 1 1    \(CP(q, d)\), the number of the co-presences of binary features, is \(2\) because of \(f_3\) and \(f_4\).
\(CA(q, d)\), the number of co-absences, is \(1\) because of \(f_1\).</description>
    </item>
    
    <item>
      <title>The Curse of Dimensionality and Feature Selection</title>
      <link>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/08-curse-of-dimensionality-and-feature-selection/</link>
      <pubDate>Thu, 09 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.c13u.com/computer-science/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/08-curse-of-dimensionality-and-feature-selection/</guid>
      <description>The Curse of Dimensionality The predictive power of an induced model is based either on:
  Partitioning the feature space into regions based on clusters of training instances and assigning a query located in region \(X\) the target value of the training instances in that cluster.
  Interpolating a target value from the target values of individual training instances that are near the query in the feature space.</description>
    </item>
    
  </channel>
</rss>