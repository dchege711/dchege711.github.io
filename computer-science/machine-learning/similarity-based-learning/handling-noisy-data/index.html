<!doctype html><html lang=en><head><title>Handling Noisy Data in Nearest Neighbors | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><svg id="background-svg"/><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/>Machine Learning & Its Applications</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/>Similarity Based Learning</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/handling-noisy-data/>Handling Noisy Data in Nearest Neighbors</a></li></ul></nav><section><header><h1>Handling Noisy Data in Nearest Neighbors</h1><p class=meta>Dated Oct 17, 2017;
last modified on Sun, 14 Mar 2021</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#majority-voting>Majority Voting</a></li><li><a href=#distance-weighting>Distance Weighting</a></li></ul></nav></aside><article id=main-article><h2 id=majority-voting>Majority Voting</h2><p>The <strong>\(k\) nearest neighbors</strong> model predicts the target level from the majority vote from the set of the \(k\) nearest neighbors to the query \(q\).</p><p>Where \(\delta\) is an indicator function such that \(\delta(t_i, l) = 1 \iff t_i = l\):</p><p>$$ \mathbb{M}_{k} (q) = argmax_{l \in levels(t)} \left( \sum_{i=1}^{k} \delta(t_i, l) \right) $$</p><p>For categorical features, \(k\) should be odd to avoid ties.</p><div class=comment-holder><div class=comment><p>This doesn&rsquo;t read right. If there are 3 possible categories, \(k = 3\) can result in a tie. &ldquo;\(k \mod |categories| \ne 0 \)&rdquo; seems like an alternative choice, but \(k = 4\) could result in \(\{2, 2, 0\}\) votes for 3 possible categories.</p></div></div><h2 id=distance-weighting>Distance Weighting</h2><p>An <strong>imbalanced dataset</strong> is one that contains significantly more of one target level than another. As \(k\) increases, the prediction will be dominated by the majority target value.</p><p>Distance-weighted \(k-NN\) presents a mitigation to imbalanced datasets:</p><p>$$ \mathbb{M}_{k} (q) = argmax_{l \in levels(t)} \left( \sum_{i=1}^{k} \frac{1}{dist(q, d_i)^2} \cdot \delta(t_i, l) \right) $$</p><p>That said, weighted \( k-NN \) is not a silver bullet for imbalanced datasets. Also, computing \( \frac{1}{dist(q, d_i)^2} \) for all \(N\) may be expensive.</p><div class=comment-holder><div class=comment><p>I don&rsquo;t follow. Why are we doing work for \(N\)? Aren&rsquo;t we interested in \(k\) instances?</p></div></div></article><div style=font-size:smaller></div></div><footer><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/caveats-on-similarity-learning/>&#171; Caveats on Similarity Learning</a>
<a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/the-case-for-range-normalization/>The Case for Range Normalization &#187;</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>