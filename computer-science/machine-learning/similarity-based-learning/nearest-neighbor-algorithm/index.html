<!doctype html><html lang=en><head><title>The Nearest Neighbor Algorithm | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><svg id="background-svg"/><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/>Machine Learning & Its Applications</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/>Similarity Based Learning</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/nearest-neighbor-algorithm/>The Nearest Neighbor Algorithm</a></li></ul></nav><section><header><h1>The Nearest Neighbor Algorithm</h1><p class=meta>Dated Oct 10, 2020;
last modified on Sun, 12 Feb 2023</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#the-algorithm>The Algorithm</a></li><li><a href=#remarks-on-the-algorithm>Remarks on the Algorithm</a></li></ul></nav></aside><article id=main-article><h2 id=the-algorithm>The Algorithm</h2><ol><li>Iterate across the instances in memory. Find the instance that has the shortest distance from the query.</li></ol><div class=comment-holder><div class=comment><p>Doing this naively is computationally expensive. That&rsquo;s why there are faster lookup methods, e.g. k-d trees.</p></div></div><ol start=2><li>Make a prediction for the query equal to the value of the target feature of the nearest neighbor</li></ol><figure><img src=/img/computer-science/ele-364-ml-and-predictive-analytics/05-similarity-based-learning/speed-agility-and-draft-status-nearest-neighbors.png alt="**The Voronoi Tessellation** is a partition of the feature space such that each partition is the 'adoptive-radius' of the instance that 'owns' that partition. The **Decision Boundary** is formed by aggregating neighboring Voronoi regions that belong to the same target level. In a \\(k-NN\\) setting, this is equivalent to setting \\(k=1\\)" loading=lazy><figcaption>**The Voronoi Tessellation** is a partition of the feature space such that each partition is the 'adoptive-radius' of the instance that 'owns' that partition. The **Decision Boundary** is formed by aggregating neighboring Voronoi regions that belong to the same target level. In a \\(k-NN\\) setting, this is equivalent to setting \\(k=1\\)</figcaption></figure><h2 id=remarks-on-the-algorithm>Remarks on the Algorithm</h2><p>Updating the model is quite cheap. Adding an instance updates the Voronoi tessellation and therefore the decision boundary.</p><p>However, because the NN algorithm is a lazy learner (doesn&rsquo;t abstract the data into a higher model), whatever savings that we make at offline time, we&rsquo;ll compensate by having longer inference times.</p><div class=comment-holder><div class=comment><p>When would anyone want to have longer inference times? It seems that reducing inference time is a crucial step for machine learning. We can do all the offline model building in fancy machines, and let even cheap machines reap the benefits of ML.</p></div></div><p>One way of making retrieval of nearest neighbors more efficient is organizing the training set into a k-d tree.</p></article><div style=font-size:smaller></div></div><footer><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/similarity-measures/>&#171; Similarity Measures</a>
<a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/caveats-on-similarity-learning/>Caveats on Similarity Learning &#187;</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>