<!doctype html><html lang=en><head><title>Similarity Based Learning | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content><meta property="og:title" content="Similarity Based Learning"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/"><meta property="og:site_name" content="curiosities.dev"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/>Machine Learning & Its Applications</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/>Similarity Based Learning</a></li></ul></nav><main><article><header><h1>Similarity Based Learning</h1><p class=meta>Dated Oct 24, 2020;
last modified on Sun, 14 Mar 2021</p></header></article><table border=0><tr class=no-decoration id=randomSelectorContainer><td class=no-decoration></td><td class=no-decoration></td><td class=no-decoration><span class=link-style onclick=goToRandomPage();>Random Link ¯\_(ツ)_/¯</span></td></tr><tr class=no-decoration><td class=no-decoration>Oct 10, 2020</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/similarity-measures/>Similarity Measures</a>
<span class=meta>2 min; updated Sep 5, 2022
<button id=9a884fe90af5e9bc85ee94b773248531-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=9a884fe90af5e9bc85ee94b773248531-summary style=display:none>To classify something, find things that are similar and label it with the same class as the most similar thing.
The feature space is \(N-d\), where \(N\) is the number of features. Each instance is mapped to a point. The descriptive features become the axes.
The Similarity Metric Mathematically, it must conform to these 4 criteria:
Non-negative: \(f(a, b) \ge 0\) Identity: \( f(a, b) = 0 \iff a = b \) Symmetry: \( f(a, b) = f(b, a) \) Triangular inequality: \( f(a, b) \le f(a, c) + f(c, b) \) Why are non-negativity and triangular inequality important?...</p></td></tr><tr class=no-decoration><td class=no-decoration>Oct 10, 2020</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/nearest-neighbor-algorithm/>The Nearest Neighbor Algorithm</a>
<span class=meta>2 min; updated Feb 12, 2023
<button id=cdabd070c00ba141b627ac58dc22c219-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=cdabd070c00ba141b627ac58dc22c219-summary style=display:none>The Algorithm Iterate across the instances in memory. Find the instance that has the shortest distance from the query. Doing this naively is computationally expensive. That&rsquo;s why there are faster lookup methods, e.g. k-d trees.
Make a prediction for the query equal to the value of the target feature of the nearest neighbor **The Voronoi Tessellation** is a partition of the feature space such that each partition is the 'adoptive-radius' of the instance that 'owns' that partition....</p></td></tr><tr class=no-decoration><td class=no-decoration>Oct 10, 2017</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/caveats-on-similarity-learning/>Caveats on Similarity Learning</a>
<span class=meta>1 min; updated Mar 14, 2021
<button id=b725840c6838e408eb91537a6cf67de0-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=b725840c6838e408eb91537a6cf67de0-summary style=display:none>Similarity-based learning is intuitive and gives people confidence in the model.
There is an inductive bias that instances that have similar descriptive features belong to the same class.
Remarkably so. When I think of classifying things, my mind immediately goes to NN.
Similarity learning has a stationary assumption, i.e. the joint PDF of the data doesn&rsquo;t change (new classifications do not come up). This assumption is shared by supervised ML....</p></td></tr><tr class=no-decoration><td class=no-decoration>Oct 17, 2017</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/handling-noisy-data/>Handling Noisy Data in Nearest Neighbors</a>
<span class=meta>1 min; updated Mar 14, 2021
<button id=be85f5ba267db192638458b598aabc1a-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=be85f5ba267db192638458b598aabc1a-summary style=display:none>Majority Voting The \(k\) nearest neighbors model predicts the target level from the majority vote from the set of the \(k\) nearest neighbors to the query \(q\).
Where \(\delta\) is an indicator function such that \(\delta(t_i, l) = 1 \iff t_i = l\):
$$ \mathbb{M}_{k} (q) = argmax_{l \in levels(t)} \left( \sum_{i=1}^{k} \delta(t_i, l) \right) $$
For categorical features, \(k\) should be odd to avoid ties.
This doesn&rsquo;t read right. If there are 3 possible categories, \(k = 3\) can result in a tie....</p></td></tr><tr class=no-decoration><td class=no-decoration>Oct 17, 2017</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/the-case-for-range-normalization/>The Case for Range Normalization</a>
<span class=meta>1 min; updated Mar 14, 2021
<button id=5abfbecf012f0c699724cf45bcfe2a92-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=5abfbecf012f0c699724cf45bcfe2a92-summary style=display:none>When you have features taking different range if values, you may have odd predictions.
For example, if \(f_1 \in [0, 100]\) and \(f_2 \in [0, 1]\), \(f_1\) will always be penalized more than \(f_2\) when computing the distance.
To mitigate this, normalize the feature&rsquo;s ranges to \([r_{low}, r_{high}]\):
$$ a'_i = \frac{a_i - a_{min}}{ a_{max} - a_{min}} \cdot (r_{high} - r_{low}) + r_{low} $$
Typically, the range is normalized to \([r_{low}, r_{high}] = [0, 1]\), so range normalization simplifies to:...</p></td></tr><tr class=no-decoration><td class=no-decoration>Oct 17, 2017</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/predicting-continuous-targets/>Predicting Continuous Targets Using NN</a>
<span class=meta>1 min; updated Mar 14, 2021
<button id=708b0eca511eecd35414ef10c689ba9b-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=708b0eca511eecd35414ef10c689ba9b-summary style=display:none>Return the Average Value One possible solution is to return the average value in the neighborhood, i.e.
$$ \mathbb{M}_{k}(q) = \frac{1}{k} \sum_{i=1}^{k} t_i $$
We can improve this by using weighted \(k-NN\):
$$ \mathbb{M}_{k}(q) = \frac{ \sum_{i=1}^{k} \left( \frac{1}{dist(q, d_i)^2} \cdot t_i \right) }{ \sum_{i=1}^{k} \frac{1}{dist(q, d_i)^2} } $$
The formula looks new. However, if \(x_1\) is weighted by \(w_1\) and \(x_2\) by \(w_2\), then the weighted average is:
$$ \frac{w_1 x_1 + w_2 x_2 }{w_1 + w_2} $$...</p></td></tr><tr class=no-decoration><td class=no-decoration>Oct 17, 2017</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/other-measures-of-similarity/>Other Measures of Similarity in NN</a>
<span class=meta>2 min; updated Feb 12, 2023
<button id=f5fecd9fcee8103a5b799fba4791d445-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=f5fecd9fcee8103a5b799fba4791d445-summary style=display:none>Note: similarity indexes may not specify the 4 NIST specifications of a similarity metric .
Consider these two instances:
\(f_1\) \(f_2\) \(f_3\) \(f_4\) \(f_5\) d 0 1 1 0 1 q 0 0 1 1 1 \(CP(q, d)\), the number of the co-presences of binary features, is \(2\) because of \(f_3\) and \(f_4\).
\(CA(q, d)\), the number of co-absences, is \(1\) because of \(f_1\)....</p></td></tr><tr class=no-decoration><td class=no-decoration>Aug 9, 2018</td><td class=no-decoration>&#187;</td><td class=no-decoration><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/curse-of-dimensionality-and-feature-selection/>The Curse of Dimensionality and Feature Selection</a>
<span class=meta>3 min; updated Mar 14, 2021
<button id=4d382feb3d4e59b20f6dde306b051dc2-summary-controller onclick=toggleSummaryDisplay(this);>
<i class="fas fa-chevron-down"></i></button></span><p class=meta id=4d382feb3d4e59b20f6dde306b051dc2-summary style=display:none>The Curse of Dimensionality The predictive power of an induced model is based either on:
Partitioning the feature space into regions based on clusters of training instances and assigning a query located in region \(X\) the target value of the training instances in that cluster.
Interpolating a target value from the target values of individual training instances that are near the query in the feature space....</p></td></tr></table><script>let pageURLs=[];const alternateOrganizations=new Set(["tags","categories","domains","authors","publications"]);fetch("/json/site_tree.json").then(response=>response.json()).then(siteTree=>{const currentPath=document.location.pathname;const effectivePath=currentPath==="/random/"?"/":currentPath;const shouldPrefixWithSlash=effectivePath==="/";let node=siteTree;effectivePath.split("/").forEach((subpath)=>{if(!subpath)return;node=node[subpath];});if(!node)return;function populateRelevantURLs(partialURL,currentNode){const currentNodeKeys=Object.keys(currentNode);currentNodeKeys.forEach((key)=>{if(key==="_meta")return;if(effectivePath==="/"&&alternateOrganizations.has(key))
return;let nextPartialURL=partialURL===""?key:`${partialURL}/${key}`;populateRelevantURLs(nextPartialURL,currentNode[key]);});if(currentNodeKeys.length===0&&partialURL){pageURLs.push(`${shouldPrefixWithSlash?"/":""}${partialURL}`);}}
populateRelevantURLs("",node);}).then(()=>{if(pageURLs.length===0){document.getElementById("randomSelectorContainer").style.display="none";}});function goToRandomPage(){let idx=Math.floor(Math.random()*(pageURLs.length));window.open(pageURLs[idx],"_self");}</script></main></div><footer><a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>