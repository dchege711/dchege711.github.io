<!doctype html><html lang=en><head><title>Other Measures of Similarity in NN | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><svg id="background-svg"/><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/>Machine Learning & Its Applications</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/>Similarity Based Learning</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/other-measures-of-similarity/>Other Measures of Similarity in NN</a></li></ul></nav><section><header><h1>Other Measures of Similarity in NN</h1><p class=meta>Dated Oct 17, 2017;
last modified on Sun, 12 Feb 2023</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#russel-rao>Russel-Rao</a></li><li><a href=#sokal-michener>Sokal-Michener</a></li><li><a href=#jaccard-index>Jaccard Index</a></li><li><a href=#cosine-similarity>Cosine Similarity</a></li><li><a href=#mahalanobis-distance>Mahalanobis Distance</a></li></ul></nav></aside><article id=main-article><p>Note: similarity indexes may not specify the
<a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/01-similarity-measures/#the-similarity-metric target=_blank rel=noopener>4 NIST specifications of a similarity metric
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.</p><div class=comment-holder><div class=comment><p>Consider these two instances:</p><table><thead><tr><th></th><th>\(f_1\)</th><th>\(f_2\)</th><th>\(f_3\)</th><th>\(f_4\)</th><th>\(f_5\)</th></tr></thead><tbody><tr><td>d</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>q</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>\(CP(q, d)\), the number of the co-presences of binary features, is \(2\) because of \(f_3\) and \(f_4\).</p><p>\(CA(q, d)\), the number of co-absences, is \(1\) because of \(f_1\).</p></div></div><h2 id=russel-rao>Russel-Rao</h2><p>Where \(|q|\) is the total number of binary features considered:</p><p>$$ f(q, d) = \frac{ CP(q, d) + CA(q, d) }{ |q| } $$</p><p>This similarity measure is used by Netflix.</p><h2 id=sokal-michener>Sokal-Michener</h2><p>$$ f(q, d) = \frac{ CP(q, d) + CA(q, d) }{ |q| } $$</p><p>This is useful for medical diagnoses.</p><h2 id=jaccard-index>Jaccard Index</h2><p>$$ f(q, d) = \frac{ CP(q, d) }{ CP(q, d) + PA(q, d) + AP(q, d) } $$</p><p>e.g. for a retailer, the list of customer-items are sparse. Co-absences are not important.</p><div class=comment-holder><div class=comment><p>It&rsquo;s probably more than that. Co-absences would dominate the predictions despite having little value.</p></div></div><h2 id=cosine-similarity>Cosine Similarity</h2><p>$$ f(q, d) = \frac{ \sum_{i=1}^{m} (q_i \cdot d_i) }{ \sqrt{ \sum_{i=1}^{m} q_i^2 } \times \sqrt{ \sum_{i=1}^{m} d_i^2 } } = \frac{\vec{q} \cdot \vec{d}}{ |\vec{q}| \times |\vec{b}| } $$</p><p>The result ranges from \(0\) (dissimilar) to \(1\) (similar).</p><p>The measure only cares about the angle between the two vectors. The magnitudes of the vectors are inconsequential.</p><div class=comment-holder><div class=comment><p>Sanity check: say we have \(q = (7, 7)\) and \(d = (2, 2)\):</p><p>$$ f(q, d) = \frac{ 7 \cdot 2 + 7 \cdot 2 }{ \sqrt{7^2 + 7^2} \times \sqrt{ 2^2 + 2^2 }} = 1 $$</p><p>Looks like cosine similarity is a trend-finder. I&rsquo;m uncomfortable that \((2, 2)\) is considered to be as close to \( (1, 1)\) as it is to \( (7, 7) \).</p></div></div><h2 id=mahalanobis-distance>Mahalanobis Distance</h2><figure><img src=/img/computer-science/ele-364-ml-and-predictive-analytics/05-similarity-based-learning/euclidean-shortcoming-vs-mahalanobis.png alt="Euclidean distance would ignore that A and B come from a similar distribution, and therefore should be regarded as more similar than A and C." loading=lazy><figcaption>Euclidean distance would ignore that A and B come from a similar distribution, and therefore should be regarded as more similar than A and C.</figcaption></figure><p>$$ f\left(\vec{a}, \vec{b}\right) = \begin{bmatrix} a_1 - b_1 & &mldr; & a_m - b_m \end{bmatrix} \times \Sigma^{-1} \times \begin{bmatrix}a_1 - b_1 \\ &mldr; \\ a_m - b_m \end{bmatrix} $$</p><p>The Mahalanobis distance scales up the distances along the direction(s) where the dataset is tightly packed.</p><p>The inverse covariance matrix, \( \Sigma^{-1} \), rescales the differences so that all features have unit variance and removes the effects of covariance.</p></article><div style=font-size:smaller></div></div><footer><a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/predicting-continuous-targets/>&#171; Predicting Continuous Targets Using NN</a>
<a href=https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/curse-of-dimensionality-and-feature-selection/>The Curse of Dimensionality and Feature Selection &#187;</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>