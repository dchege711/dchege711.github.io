<!DOCTYPE html>
<html>

    <head>
        <title>
             
                Other Measures of Similarity in NN | c13u
            
        </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" type="text/css" href="/css/main.css" />
        <link rel="stylesheet" type="text/css" href="/css/all_font_awesome_v5.9.min.css" />
        
        <link rel="shortcut icon" href="/img/favicon_io/favicon.ico">
        <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon_io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon_io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon_io/favicon-16x16.png">

        <link rel="stylesheet" href="/css/vs.css">
        <script type="text/javascript" src="/js/highlight.pack.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script type="text/javascript" src="/js/d3/d3.min.js"></script>
        <script type="text/javascript" src="/js/PlotUtils.js"></script>
        <script type="text/javascript" src="/js/OrganizeCitations.js"></script>
        <script type="text/javascript" src="/js/HighlightAnchor.js"></script>

        
        
    </head>

    <body>

        <div class="container" id="main_div">

            
            <form action="/search" method="get" id="globalSearchForm">
                <input type="text" id="q" name="q">
                <input type="submit" id="submitButton" value="Search">
            </form>
            
            
            
            <nav aria-label="Breadcrumb" class="breadcrumb">
    <ul>
        









<li>
  <a href="https://www.c13u.com/">Home</a>
</li>


<li>
  <a href="https://www.c13u.com/computer-science/">Computer Science &amp; Software Engineering</a>
</li>


<li>
  <a href="https://www.c13u.com/computer-science/machine-learning/">Machine Learning &amp; Its Applications</a>
</li>


<li>
  <a href="https://www.c13u.com/computer-science/machine-learning/similarity-based-learning/">Similarity Based Learning</a>
</li>


<li class="active">
  <a href="https://www.c13u.com/computer-science/machine-learning/similarity-based-learning/other-measures-of-similarity/">Other Measures of Similarity in NN</a>
</li>

    </ul>
</nav>


            
            
<section>
    <header>
    <h1> Other Measures of Similarity in NN</h1>
    <p class="meta">
        
        Dated Oct 17, 2017; 
        
        last modified on Sun, 14 Mar 2021
        
    </p>
    </header>

    <div id="toc-then-article">
        <aside id="toc">
            <nav id="TableOfContents">
  <ul>
    <li><a href="#russel-rao">Russel-Rao</a></li>
    <li><a href="#sokal-michener">Sokal-Michener</a></li>
    <li><a href="#jaccard-index">Jaccard Index</a></li>
    <li><a href="#cosine-similarity">Cosine Similarity</a></li>
    <li><a href="#mahalanobis-distance">Mahalanobis Distance</a></li>
  </ul>
</nav>
        </aside>

        <article id="main-article">
            <p>Note: similarity indexes may not specify the 
<a href="https://www.c13u.com/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/01-similarity-measures/#the-similarity-metric"
    
    
    
    >
    4 NIST specifications of a similarity metric
    
</a>.</p>



<div class="comment-holder">
    <div class="comment"><p>Consider these two instances:</p>
<table>
<thead>
<tr>
<th></th>
<th>\(f_1\)</th>
<th>\(f_2\)</th>
<th>\(f_3\)</th>
<th>\(f_4\)</th>
<th>\(f_5\)</th>
</tr>
</thead>
<tbody>
<tr>
<td>d</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>q</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>\(CP(q, d)\), the number of the co-presences of binary features, is \(2\) because of \(f_3\) and \(f_4\).</p>
<p>\(CA(q, d)\), the number of co-absences, is \(1\) because of \(f_1\).</p>
</div>
</div>


<h2 id="russel-rao">Russel-Rao</h2>
<p>Where \(|q|\) is the total number of binary features considered:</p>
<p>$$ f(q, d) = \frac{ CP(q, d) + CA(q, d) }{ |q| } $$</p>
<p>This similarity measure is used by Netflix.</p>
<h2 id="sokal-michener">Sokal-Michener</h2>
<p>$$ f(q, d) = \frac{ CP(q, d) + CA(q, d) }{ |q| } $$</p>
<p>This is useful for medical diagnoses.</p>
<h2 id="jaccard-index">Jaccard Index</h2>
<p>$$ f(q, d) = \frac{ CP(q, d) }{ CP(q, d) + PA(q, d) + AP(q, d) } $$</p>
<p>e.g. for a retailer, the list of customer-items are sparse. Co-absences are not important.</p>



<div class="comment-holder">
    <div class="comment"><p>It&rsquo;s probably more than that. Co-absences would dominate the predictions despite having little value.</p>
</div>
</div>


<h2 id="cosine-similarity">Cosine Similarity</h2>
<p>$$ f(q, d) = \frac{ \sum_{i=1}^{m} (q_i \cdot d_i) }{ \sqrt{ \sum_{i=1}^{m} q_i^2 } \times \sqrt{ \sum_{i=1}^{m} d_i^2 } } = \frac{\vec{q} \cdot \vec{d}}{ |\vec{q}| \times |\vec{b}| } $$</p>
<p>The result ranges from \(0\) (dissimilar) to \(1\) (similar).</p>
<p>The measure only cares about the angle between the two vectors. The magnitudes of the vectors are inconsequential.</p>



<div class="comment-holder">
    <div class="comment"><p>Sanity check: say we have \(q = (7, 7)\) and \(d = (2, 2)\):</p>
<p>$$ f(q, d) = \frac{ 7 \cdot 2 + 7 \cdot 2 }{ \sqrt{7^2 + 7^2} \times \sqrt{ 2^2 + 2^2 }} = 1 $$</p>
<p>Looks like cosine similarity is a trend-finder. I&rsquo;m uncomfortable that \((2, 2)\) is considered to be as close to \( (1, 1)\) as it is to \( (7, 7) \).</p>
</div>
</div>


<h2 id="mahalanobis-distance">Mahalanobis Distance</h2>
<figure>
    <img src="/img/computer-science/ele-364-ml-and-predictive-analytics/05-similarity-based-learning/euclidean-shortcoming-vs-mahalanobis.png"
         alt="Euclidean distance would ignore that A and B come from a similar distribution, and therefore should be regarded as more similar than A and C."/> <figcaption>
            <p>Euclidean distance would ignore that A and B come from a similar distribution, and therefore should be regarded as more similar than A and C.</p>
        </figcaption>
</figure>

<p>$$ f\left(\vec{a}, \vec{b}\right) = \begin{bmatrix} a_1 - b_1 &amp; &hellip; &amp; a_m - b_m \end{bmatrix} \times \Sigma^{-1} \times \begin{bmatrix}a_1 - b_1 \\ &hellip; \\ a_m - b_m \end{bmatrix} $$</p>
<p>The Mahalanobis distance scales up the distances along the direction(s) where the dataset is tightly packed.</p>
<p>The inverse covariance matrix, \( \Sigma^{-1} \), rescales the differences so that all features have unit variance and removes the effects of covariance.</p>

        </article>

        




    </div>
    <footer>
        
        
        
            
        

        
            <a href="https://www.c13u.com/computer-science/machine-learning/similarity-based-learning/predicting-continuous-targets/">&laquo; Predicting Continuous Targets Using NN</a>
        
        
            <a href="https://www.c13u.com/computer-science/machine-learning/similarity-based-learning/curse-of-dimensionality-and-feature-selection/">The Curse of Dimensionality and Feature Selection &raquo;</a>
        

    </footer>
</section>


        </div>

        <footer>
            <a href="mailto:d.chege711@gmail.com">Email</a>
            
            <a href="/about">About</a>
            <a href="/search">Search</a>
        </footer>

    </body>

</html>
