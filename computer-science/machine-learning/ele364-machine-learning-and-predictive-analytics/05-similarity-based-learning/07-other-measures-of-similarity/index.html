<!doctype html><html lang=en><head><title>Other Measures of Similarity in NN | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="This list is not exhaustive. For example,  lists multiple distance and similarity measures for different kinds of data: numerical (12), boolean (8), string (5), images & color (2), geospatial & temporal (4), and general & mixed (1).
Nominal variables are variables that have two or more categories, but which do not have an intrinsic order. Dichotomous variables are nominal variables which have only two categories. 
Dichotomous attributes (e.g. yes-or-no) are distinct from binary attributes (present vs...."><meta property="og:title" content="Other Measures of Similarity in NN"><meta property="og:description" content="This list is not exhaustive. For example,  lists multiple distance and similarity measures for different kinds of data: numerical (12), boolean (8), string (5), images & color (2), geospatial & temporal (4), and general & mixed (1).
Nominal variables are variables that have two or more categories, but which do not have an intrinsic order. Dichotomous variables are nominal variables which have only two categories. 
Dichotomous attributes (e.g. yes-or-no) are distinct from binary attributes (present vs...."><meta property="og:type" content="website"><meta property="og:url" content="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/07-other-measures-of-similarity/"><meta property="og:site_name" content="curiosities.dev"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/>Machine Learning & Its Applications</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/>Machine Learning & Predictive Analytics [ELE 364]</a></li><li><a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/>Similarity Based Learning</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/07-other-measures-of-similarity/>Other Measures of Similarity in NN</a></li></ul></nav><section><header><h1>Other Measures of Similarity in NN</h1><p class=meta>Dated Oct 17, 2017;
last modified on Sun, 12 Feb 2023</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#russell-rao>Russell-Rao</a></li><li><a href=#sokal-michener>Sokal-Michener</a></li><li><a href=#jaccard-index>Jaccard Index</a></li><li><a href=#cosine-similarity>Cosine Similarity</a></li><li><a href=#mahalanobis-distance>Mahalanobis Distance</a></li><li><a href=#references>References</a></li></ul></nav></aside><article id=main-article><p>This list is not exhaustive. For example, <span class=citation-ref><a href=#WolframSimilarity></a></span>lists
multiple distance and similarity measures for different kinds of data: numerical
(12), boolean (8), string (5), images & color (2), geospatial & temporal (4),
and general & mixed (1).</p><div class=comment-holder><div class=comment><p>Nominal variables are variables that have two or more categories, but which do
not have an intrinsic order. Dichotomous variables are nominal variables which
have only two categories. <span class=citation-ref><a href=#LaerdVariableTypes></a></span></p><p>Dichotomous attributes (e.g. yes-or-no) are distinct from binary attributes
(present vs. absent), e.g. binary attributes may be asymmetric in that
co-presence suggests similarity, but co-absence may or may not be considered
evidence of similarity. <span class=citation-ref><a href=#StacksExch2013></a></span>.</p></div></div><div class=comment-holder><div class=comment><p>Given the multiple options, I think it&rsquo;s more useful to know when to apply a
specific similarity measure. The exact definitions themselves are not as
important as we can look them up if need be, but the rationales are helpful in
figuring out what to consider. The rationales can be gleaned from inspecting the
formulas, e.g. <span class=citation-ref><a href=#StacksExch2013></a></span>.</p></div></div><p>Note: similarity indexes may not specify the
<a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/01-similarity-measures/#the-similarity-metric target=_blank rel=noopener>4 NIST specifications of a
similarity metric
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>: non-negativity, identity, symmetry, and subaddivity.<div class=comment-holder><div class=comment><h3 id=definitions-of-cp-and-ca>Definitions of \(CP\) and \(CA\)</h3><table><thead><tr><th></th><th>\(f_1\)</th><th>\(f_2\)</th><th>\(f_3\)</th><th>\(f_4\)</th><th>\(f_5\)</th></tr></thead><tbody><tr><td>d</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr><tr><td>q</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr></tbody></table><p>\(CP(q, d)\), the number of the co-presences of binary features, is \(2\) because of \(f_3\) and \(f_4\).</p><p>\(CA(q, d)\), the number of co-absences, is \(1\) because of \(f_1\).</p></div></div></p><h2 id=russell-rao>Russell-Rao</h2><div class=comment-holder><div class=comment><p>Kinda confusing for \(q\) to mean query in \(f(q, d)\), and something else
in \(|q|\).</p></div></div><p>Where \(|q|\) is the total number of binary features considered:</p><p>$$ f(q, d) = \frac{ CP(q, d) }{ |q| } $$</p><p><a href=#jaccard-index>Jaccard</a>
is more popular than Russell-Rao because datasets tend
to be sparse that \(|q|\) ends up dominating. <span class=citation-ref><a href=#StacksExch2013></a></span></p><div class=comment-holder><div class=comment><p>I keep seeing references to &ldquo;dissimilarity measures&rdquo; and not much of &ldquo;similarity
measures&rdquo;. Not sure why a difference in nomenclature exists.</p><p><del>For instance, for measure \(m = F(a, b)\), it&rsquo;s not as if when \(a\) and
\(b\) are similar, then \(m \to 0\) for a dissimilarity measure \(F\),
while \(m > 0\) (usually \(m \to 1\)) for a similarity measure \(F\).</del></p><p><span class=citation-ref><a href=#Zhang2003></a></span>defines both, illustrating a difference. For example, the
Russell-Rao similarity measure is \(\frac{S_{11}}{N}\), while the
dissimilarity measure is \(\frac{N - S_{11}}{N}\). \(S_{11}\) is the number
of occurrences of matches with ones in corresponding locations.</p></div></div><h2 id=sokal-michener>Sokal-Michener</h2><p>$$ f(q, d) = \frac{ CP(q, d) + CA(q, d) }{ |q| } $$</p><p>Useful for medical diagnoses.</p><h2 id=jaccard-index>Jaccard Index</h2><p>$$ f(q, d) = \frac{ CP(q, d) }{ CP(q, d) + PA(q, d) + AP(q, d) } $$</p><p>Useful when the data is sparse, and co-absences are not important, e.g. a
retailer&rsquo;s list of customer-items.</p><h2 id=cosine-similarity>Cosine Similarity</h2><p>$$ f(q, d) = \frac{ \sum_{i=1}^{m} (q_i \cdot d_i) }{ \sqrt{ \sum_{i=1}^{m} q_i^2 } \times \sqrt{ \sum_{i=1}^{m} d_i^2 } } = \frac{\vec{q} \cdot \vec{d}}{ |\vec{q}| \times |\vec{b}| } $$</p><p>The result ranges from \(0\) (dissimilar) to \(1\) (similar).</p><p>The measure only cares about the angle between the two vectors. The magnitudes of the vectors are inconsequential.</p><div class=comment-holder><div class=comment><p>Sanity check: say we have \(q = (7, 7)\) and \(d = (2, 2)\):</p><p>$$ f(q, d) = \frac{ 7 \cdot 2 + 7 \cdot 2 }{ \sqrt{7^2 + 7^2} \times \sqrt{ 2^2 + 2^2 }} = 1 $$</p><p>Looks like cosine similarity is a trend-finder. I&rsquo;m uncomfortable that \((2, 2)\) is considered to be as close to \( (1, 1)\) as it is to \( (7, 7) \).</p></div></div><h2 id=mahalanobis-distance>Mahalanobis Distance</h2><figure><img src=/img/computer-science/ele-364-ml-and-predictive-analytics/05-similarity-based-learning/euclidean-shortcoming-vs-mahalanobis.png alt="Euclidean distance would ignore that A and B come from a similar distribution, and therefore should be regarded as more similar than A and C." loading=lazy><figcaption>Euclidean distance would ignore that A and B come from a similar distribution, and therefore should be regarded as more similar than A and C.</figcaption></figure><p>$$ f\left(\vec{a}, \vec{b}\right) = \begin{bmatrix} a_1 - b_1 & &mldr; & a_m - b_m \end{bmatrix} \times \Sigma^{-1} \times \begin{bmatrix}a_1 - b_1 \\ &mldr; \\ a_m - b_m \end{bmatrix} $$</p><p>The Mahalanobis distance scales up the distances along the direction(s) where the dataset is tightly packed.</p><p>The inverse covariance matrix, \( \Sigma^{-1} \), rescales the differences so that all features have unit variance and removes the effects of covariance.</p><h2 id=references>References</h2><ol><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=WolframSimilarity>Distance and Similarity Measures<i>.</i></cite>
<a href=https://reference.wolfram.com/language/guide/DistanceAndSimilarityMeasures.html target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=reference.wolfram.com" loading=lazy aria-hidden=true width=16 height=16>
<i>reference.wolfram.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Oct 27, 2021.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count is-main><cite id=Zhang2003>Properties of Binary Vector Dissimilarity Measures<i>.</i></cite>
Bin Zhang; Sargur N. Srihari.
JCIS International Conf. Computer Vision, Pattern Recognition, and Image Processing, Vol. 1, 2003.
<a href=https://cedar.buffalo.edu/papers/articles/CVPRIP03_propbina.pdf target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=cedar.buffalo.edu" loading=lazy aria-hidden=true width=16 height=16>
<i>cedar.buffalo.edu</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
2003.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i></div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=StacksExch2013>Similarity Coefficients for binary data: Why choose Jaccard over Russell and Rao?<i></i></cite>
<a href=https://stats.stackexchange.com/a/61910 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=stats.stackexchange.com" loading=lazy aria-hidden=true width=16 height=16>
<i>stats.stackexchange.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Oct 28, 2021.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=LaerdVariableTypes>Understanding the different types of variable in statistics > Categorical and Continuous Variables<i>.</i></cite>
<a href="https://statistics.laerd.com/statistical-guides/types-of-variable.php#:~:text=Categorical%20and%20Continuous%20Variables" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=statistics.laerd.com" loading=lazy aria-hidden=true width=16 height=16>
<i>statistics.laerd.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Oct 28, 2021.</div></li></ol></article><div style=font-size:smaller><aside id=authors-holder style="margin:0 0 2%">Cited Authors:
<a href=/cited-authors/Srihari-Sargur-N.>Srihari, Sargur N.</a>
<a href=/cited-authors/Zhang-Bin>Zhang, Bin</a></aside><aside id=publications-holder style="margin:0 0 2%">Cited Publications:
<a href=/publications/JCIS-International-Conf.-Computer-Vision-Pattern-Recognition-and-Image-Processing>JCIS International Conf. Computer Vision, Pattern Recognition, and Image Processing</a></aside><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/cedar.buffalo.edu style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=cedar.buffalo.edu" loading=lazy aria-hidden=true width=16 height=16>
cedar.buffalo.edu</a>
<a href=/domains/reference.wolfram.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=reference.wolfram.com" loading=lazy aria-hidden=true width=16 height=16>
reference.wolfram.com</a>
<a href=/domains/statistics.laerd.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=statistics.laerd.com" loading=lazy aria-hidden=true width=16 height=16>
statistics.laerd.com</a>
<a href=/domains/stats.stackexchange.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=stats.stackexchange.com" loading=lazy aria-hidden=true width=16 height=16>
stats.stackexchange.com</a></aside></div></div><footer><a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/06-predicting-continuous-targets/>&#171; Predicting Continuous Targets Using NN</a>
<a href=https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/08-curse-of-dimensionality-and-feature-selection/>The Curse of Dimensionality and Feature Selection &#187;</a></footer></section></div><footer><a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>