<!DOCTYPE html>
<html lang="en">

    <head>
        <title>
            
                Similarity Based Learning | curiosities.dev
            
        </title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" type="text/css" href="/css/main.css" />
        <link rel="stylesheet" type="text/css" href="/css/all_font_awesome_v5.9.min.css" />
        
        <link rel="shortcut icon" href="/img/favicon_io/favicon.ico">
        <link rel="apple-touch-icon" sizes="180x180" href="/img/favicon_io/apple-touch-icon.png">
        <link rel="icon" type="image/png" sizes="32x32" href="/img/favicon_io/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/img/favicon_io/favicon-16x16.png">

        <link rel="stylesheet" href="/css/vs.css">
        <script type="text/javascript" src="/js/highlight.min.js"></script>
        <script>
            
            const hjlsURLRegex = /https?:\/\/[^\s<]+/g
            hljs.addPlugin({
                "after:highlight": (result) => {
                    result.value = result.value.replaceAll(
                        hjlsURLRegex, "<a href='$&' target='_blank'>$&</a>");
                }
            });
            hljs.highlightAll();
        </script>

        <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
        </script>

        <script type="text/javascript" src="/js/d3/d3.min.js"></script>
        <script type="text/javascript" src="/js/PlotUtils.js"></script>
        <script type="text/javascript" src="/js/OrganizeCitations.js"></script>
        <script type="text/javascript" src="/js/HighlightAnchor.js"></script>

        
    <script type="text/javascript" src="/js/ListPageUtils.js"></script>

    </head>

    <body>

        <div class="container" id="main_div">

            
            <form action="/search" method="get" id="globalSearchForm">
                <input type="text" id="q" name="q" title="Search Query">
                <input type="submit" id="submitButton" value="Search">
            </form>

            

            <nav aria-label="Breadcrumb" class="breadcrumb">
    <ul>
        









<li>
  <a href="https://www.curiosities.dev/">Home</a>
</li>


<li>
  <a href="https://www.curiosities.dev/computer-science/">Computer Science &amp; Software Engineering</a>
</li>


<li>
  <a href="https://www.curiosities.dev/computer-science/machine-learning/">Machine Learning &amp; Its Applications</a>
</li>


<li>
  <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/">Machine Learning &amp; Predictive Analytics [ELE 364]</a>
</li>


<li class="active">
  <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/">Similarity Based Learning</a>
</li>

    </ul>
</nav>



            
<main>
    <article>
        <header>
            <h1>Similarity Based Learning</h1>
            <p class="meta">
                
                Dated Oct 10, 2017; 
                
                last modified on Sun, 14 Mar 2021
                
            </p>
        </header>
        
        
    </article>

    
    <table border="0">
        <tr class="no-decoration" id="randomSelectorContainer">
            <td class="no-decoration"></td>
            <td class="no-decoration"></td>
            <td class="no-decoration"><span class="link-style" onclick="goToRandomPage();">Random Link ¯\_(ツ)_/¯</span></td>
        </tr>
        
        
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 10, 2020</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/01-similarity-measures/">Similarity Measures</a>
            
                
                    <span class="meta">
                    2 min; updated Nov 6, 2021 
                    <button 
                        id="645bb7f3805e08ae52befed25802b597-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="645bb7f3805e08ae52befed25802b597-summary" style="display: none;">
                        To classify something, find things that are similar and label it with the same class as the most similar thing.
The feature space is \(N-d\), where \(N\) is the number of features. Each instance is mapped to a point. The descriptive features become the axes.
The Similarity Metric Mathematically, it must conform to these 4 criteria:
 Non-negativity: \(f(a, b) \ge 0\) Identity of Indiscernables: \( f(a, b) = 0 \iff a = b \) Symmetry: \( f(a, b) = f(b, a) \) Subaddivity (Triangular inequality): \( f(a, b) \le f(a, c) + f(c, b) \)  Why are non-negativity and triangular inequality important?...
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 10, 2020</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/02-the-nearest-neighbor-algorithm/">The Nearest Neighbor Algorithm</a>
            
                
                    <span class="meta">
                    2 min; updated Mar 14, 2021 
                    <button 
                        id="74e7bba81f729bef74317b3df5ef7efe-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="74e7bba81f729bef74317b3df5ef7efe-summary" style="display: none;">
                        The Algorithm  Iterate across the instances in memory. Find the instance that has the shortest distance from the query.  Doing this naively is computationally expensive. That&rsquo;s why there are faster lookup methods, e.g. k-d trees.
  Make a prediction for the query equal to the value of the target feature of the nearest neighbor    The Voronoi Tessellation is a partition of the feature space such that each partition is the &lsquo;adoptive-radius&rsquo; of the instance that &lsquo;owns&rsquo; that partition....
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 10, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/03-caveats-on-similarity-learning/">Caveats on Similarity Learning</a>
            
                
                    <span class="meta">
                    1 min; updated Mar 14, 2021 
                    <button 
                        id="94cf47af7ce0f5eec2e942dcc221ff6d-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="94cf47af7ce0f5eec2e942dcc221ff6d-summary" style="display: none;">
                        Similarity-based learning is intuitive and gives people confidence in the model.
There is an inductive bias that instances that have similar descriptive features belong to the same class.
Remarkably so. When I think of classifying things, my mind immediately goes to NN.
  Similarity learning has a stationary assumption, i.e. the joint PDF of the data doesn&rsquo;t change (new classifications do not come up). This assumption is shared by supervised ML....
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/04-handling-noisy-data/">Handling Noisy Data in Nearest Neighbors</a>
            
                
                    <span class="meta">
                    1 min; updated Mar 14, 2021 
                    <button 
                        id="5d20ae7c5780f90089c86dcbcd838dcb-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="5d20ae7c5780f90089c86dcbcd838dcb-summary" style="display: none;">
                        Majority Voting The \(k\) nearest neighbors model predicts the target level from the majority vote from the set of the \(k\) nearest neighbors to the query \(q\).
Where \(\delta\) is an indicator function such that \(\delta(t_i, l) = 1 \iff t_i = l\):
$$ \mathbb{M}_{k} (q) = argmax_{l \in levels(t)} \left( \sum_{i=1}^{k} \delta(t_i, l) \right) $$
For categorical features, \(k\) should be odd to avoid ties.
This doesn&rsquo;t read right. If there are 3 possible categories, \(k = 3\) can result in a tie....
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/05-the-case-for-range-normalization/">The Case for Range Normalization</a>
            
                
                    <span class="meta">
                    1 min; updated Mar 14, 2021 
                    <button 
                        id="247a60ea547e82b4e79478b61a514d95-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="247a60ea547e82b4e79478b61a514d95-summary" style="display: none;">
                        When you have features taking different range if values, you may have odd predictions.
For example, if \(f_1 \in [0, 100]\) and \(f_2 \in [0, 1]\), \(f_1\) will always be penalized more than \(f_2\) when computing the distance.
  To mitigate this, normalize the feature&rsquo;s ranges to \([r_{low}, r_{high}]\):
$$ a'_i = \frac{a_i - a_{min}}{ a_{max} - a_{min}} \cdot (r_{high} - r_{low}) + r_{low} $$
Typically, the range is normalized to \([r_{low}, r_{high}] = [0, 1]\), so range normalization simplifies to:...
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/06-predicting-continuous-targets/">Predicting Continuous Targets Using NN</a>
            
                
                    <span class="meta">
                    1 min; updated Mar 14, 2021 
                    <button 
                        id="f855aa24189c6c7b9ad2dcdbcf291c56-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="f855aa24189c6c7b9ad2dcdbcf291c56-summary" style="display: none;">
                        Return the Average Value One possible solution is to return the average value in the neighborhood, i.e.
$$ \mathbb{M}_{k}(q) = \frac{1}{k} \sum_{i=1}^{k} t_i $$
We can improve this by using weighted \(k-NN\):
$$ \mathbb{M}_{k}(q) = \frac{ \sum_{i=1}^{k} \left( \frac{1}{dist(q, d_i)^2} \cdot t_i \right) }{ \sum_{i=1}^{k} \frac{1}{dist(q, d_i)^2} } $$
The formula looks new. However, if \(x_1\) is weighted by \(w_1\) and \(x_2\) by \(w_2\), then the weighted average is:
$$ \frac{w_1 x_1 + w_2 x_2 }{w_1 + w_2} $$...
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Oct 17, 2017</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/07-other-measures-of-similarity/">Other Measures of Similarity in NN</a>
            
                
                    <span class="meta">
                    4 min; updated Oct 28, 2021 
                    <button 
                        id="5de4ddcdd9578dda56ca64cd0266fde6-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="5de4ddcdd9578dda56ca64cd0266fde6-summary" style="display: none;">
                        This list is not exhaustive. For example,  lists multiple distance and similarity measures for different kinds of data: numerical (12), boolean (8), string (5), images &amp; color (2), geospatial &amp; temporal (4), and general &amp; mixed (1).
Nominal variables are variables that have two or more categories, but which do not have an intrinsic order. Dichotomous variables are nominal variables which have only two categories. 
Dichotomous attributes (e.g. yes-or-no) are distinct from binary attributes (present vs....
                    </p>
                
            
        </td>
    </tr>


                
            
                
                    
    <tr class="no-decoration">
        <td class="no-decoration">Aug 9, 2018</td>
        <td class="no-decoration">&raquo; </td>
        <td class="no-decoration">
            <a href="https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/08-curse-of-dimensionality-and-feature-selection/">The Curse of Dimensionality and Feature Selection</a>
            
                
                    <span class="meta">
                    3 min; updated Mar 14, 2021 
                    <button 
                        id="2e2baf184c6d9b01ef14d32b64c23632-summary-controller"
                        onclick="toggleSummaryDisplay(this);">
                            <i class="fas fa-chevron-down"></i>
                    </button>
                    </span>
                    <p class="meta" id="2e2baf184c6d9b01ef14d32b64c23632-summary" style="display: none;">
                        The Curse of Dimensionality The predictive power of an induced model is based either on:
  Partitioning the feature space into regions based on clusters of training instances and assigning a query located in region \(X\) the target value of the training instances in that cluster.
  Interpolating a target value from the target values of individual training instances that are near the query in the feature space....
                    </p>
                
            
        </td>
    </tr>


                
            
        
    </table>

    <script>
    let pageURLs = [];
    const alternateOrganizations = new Set([
        "tags", "categories", "domains", "authors", "publications"]);
    fetch("/json/site_tree.json")
        .then(response => response.json())
        .then(siteTree => {
            const currentPath = document.location.pathname;
            let node = siteTree;
            currentPath.split("/").forEach((subpath) => {
                if (!subpath) return;
                node = node[subpath];
            });
            if (!node) return;

            function populateRelevantURLs(partialURL, currentNode) {
                const currentNodeKeys = Object.keys(currentNode);
                currentNodeKeys.forEach((key) => {
                    
                    if (key === "_meta") return;

                    
                    
                    
                    
                    
                    if (currentPath === "/" && alternateOrganizations.has(key))
                        return;

                    
                    
                    let nextPartialURL = partialURL === ""
                                            ? key
                                            : `${partialURL}/${key}`;

                    populateRelevantURLs(nextPartialURL, currentNode[key]);
                });

                // A node that has no children is a regular page, and therefore
                
                
                if (currentNodeKeys.length === 0 && partialURL) {
                    pageURLs.push(partialURL);
                }
            }
            populateRelevantURLs("", node);
        })
        .then(() => {
            
            
            
            
            if (pageURLs.length === 0) {
                document.getElementById("randomSelectorContainer").style.display = "none";
            }
        });

    function goToRandomPage() {
        
        
        let idx = Math.floor(Math.random() * (pageURLs.length));
        window.open(pageURLs[idx], "_self");
    }
</script>

    
</main>


        </div>

        <footer>
            <a href="mailto:d.chege711@gmail.com">Email</a>
            
            <a href="/about">About</a>
            <a href="/search">Search</a>
        </footer>

    </body>

</html>
