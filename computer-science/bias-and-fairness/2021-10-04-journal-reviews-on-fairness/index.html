<!doctype html><html lang=en><head><title>Journal Reviews on Fairness | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="Meta ðŸ“‘    Instead of changing the data or learners in multiple ways and then see if fairness improves,  postulate that the root causes of bias are the prior decisions that generated the training data. These affect (a) what data was selected, and (b) the labels assigned to the examples. They propose the \(\text{Fair-SMOTE}\) (Fair Synthetic Minority Over Sampling Technique) algorithm which (1) removes biased labels (via situation testing: if the model&rsquo;s prediction for a data point changes once all of the data points' protected attributes  are flipped, then that label is biased and the data point is discarded), and (2) rebalances internal distributions such that based on a protected attribute, examples are equal in both positive and negative classes...."><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><svg id="background-svg"/><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/bias-and-fairness/>Computational Bias and Fairness</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/bias-and-fairness/2021-10-04-journal-reviews-on-fairness/>Journal Reviews on Fairness</a></li></ul></nav><section><header><h1>Journal Reviews on Fairness</h1><p class=meta>Dated Oct 4, 2021;
last modified on Sun, 12 Feb 2023</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#meta>Meta</a></li><li><a href=#sense-making>Sense-making</a></li><li><a href=#gender-bias>Gender Bias</a></li><li><a href=#recommendation-systems>Recommendation Systems</a></li><li><a href=#policy>Policy</a></li><li><a href=#glossary>Glossary</a></li><li><a href=#references>References</a></li></ul></nav></aside><article id=main-article><h2 id=meta>Meta</h2><div class=flashcard><div class=flashcard-link-holder id=card-61f033e5a31a800004e588a4><a href="https://cards.c13u.com/browse/?cardID=61f033e5a31a800004e588a4" title="Flashcard: Chakraborty2021 postulate that the root causes of bias are the prior decisions that generated the training data. What 2 issues do these prior decisions bring, and how do Chakraborty2021 resolve them?" target=_blank><span aria-hidden=true>ðŸ“‘</span></a></div><p>Instead of changing the data or learners in multiple ways and then see if
fairness improves, <span class=citation-ref><a href=#Chakraborty2021></a></span>postulate that the root causes
of bias are the prior decisions that generated the training data. These affect
(a) what data was selected, and (b) the labels assigned to the examples. They
propose the \(\text{Fair-SMOTE}\) (Fair Synthetic Minority Over Sampling
Technique) algorithm which (1) removes biased labels (via <strong>situation testing:</strong>
if the model&rsquo;s prediction for a data point changes once all of the data points'
<a href=#ProtectedAttributes>protected attributes</a>
are flipped, then that label is
biased and the data point is discarded), and (2) rebalances internal
distributions such that based on a protected attribute, examples are equal in
both positive and negative classes. The method is just as effective in reducing
bias as prior approaches, <em>and</em> its models achieve higher recall and F1
performance. Furthermore, \(\text{Fair-SMOTE}\) can simultaneously reduce bias
for more than one protected attribute.</p></div><div class=comment-holder><div class=comment><p><span class=citation-ref><a href=#Chakraborty2021></a></span>also claim that they are one of the largest studies
on bias mitigation yet presented. Data sets considered:
<a href=https://archive.ics.uci.edu/ml/datasets/adult target=_blank rel=noopener>UCI
Adult
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://github.com/propublica/compas-analysis target=_blank rel=noopener>Compas
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://archive.ics.uci.edu/ml/datasets/statlog+%28german+credit+data%29 target=_blank rel=noopener>Statlog (German Credit
Data)
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients target=_blank rel=noopener>Default of Credit Card
Clients
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://archive.ics.uci.edu/ml/datasets/Heart+Disease target=_blank rel=noopener>Heart Disease
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://www.kaggle.com/c/bank-marketing-uci target=_blank rel=noopener>Bank
Marketing UCI
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://www.kaggle.com/c/home-credit-default-risk target=_blank rel=noopener>Home Credit
Default Risk
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>,
<a href=https://archive.ics.uci.edu/ml/datasets/Student+Performance target=_blank rel=noopener>Student
Performance
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>, and
<a href=https://meps.ahrq.gov/mepsweb/ target=_blank rel=noopener>Medical Expenditure Panel Survey
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.</p></div></div><p>To overcome the &ldquo;It is impossible to achieve fairness and high performance
simultaneously (except in trivial cases)&rdquo; truism, <span class=citation-ref><a href=#Chakraborty2021></a></span>propose that extrapolating all variables by the same amount allows bias
mitigation while maintaining/improving performance.</p><div class=comment-holder><div class=comment><p><span class=citation-ref><a href=#Chakraborty2021></a></span>is quite bold in their claims. Watch this space!</p></div></div><h2 id=sense-making>Sense-making</h2><div class=comment-holder><div class=comment><p><span class=citation-ref><a href=#Gu2021></a></span>define sense-making as the process that humans employ to
construct meaning from raw data.</p></div></div><p><a id=SilvaDiscussion></a><span class=citation-ref><a href=#Yan2020></a></span>introduce Silva, an interactive
system that helps users reason about unfairness in ML applications. Silva
integrates a causality viewer to help identify the influence of potential bias,
multi-group comparisons to help users compare subsets of data, and a
visualization of metrics to quantify potential bias.</p><figure><img src=/img/computer-science/bias-and-fairness/yan-silva.jpg alt="Silva's main interface. Credits: Yan2020" loading=lazy><figcaption>Silva's main interface. Credits: Yan2020</figcaption></figure><div class=comment-holder><div class=comment><p>Time spent lurking HN makes me think that a theory for causality is yet to
achieve consensus, e.g.
<a href="https://news.ycombinator.com/item?id=24487135" target=_blank rel=noopener>The Book of Why: The New Science of Cause and Effect
[pdf] | Hacker News
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.</p><p><span class=citation-ref><a href=#Yan2020></a></span>are in the &ldquo;causality expressed as directed acyclic graph
(DAG)&rdquo; camp. They define causal fairness as there being no path from sensitive /
protected attributes to outcome variables in the DAG.</p></div></div><p>Through logs, think-aloud data and semi-structured interviews, <span class=citation-ref><a href=#Gu2021></a></span>explore how various interactive de-biasing tool affordances affect
sense-making. Tools should test and account for the skill level of their users,
e.g. users who didn&rsquo;t understand
<a href=#SilvaDiscussion>Silva&rsquo;s</a>
causal graph
reported it not being useful, while users who were presented with
recommendations may be subject to bias if untrained. Although combining
exploration and recommendation risks choice overload, a hybrid approach may be
useful, e.g. reducing switching costs, connecting individual UI components, and
strategically offering recommendation when bootstrapping is involved. Exposing
more model parameters, e.g. in
<a href=https://pair-code.github.io/what-if-tool/ target=_blank rel=noopener>Google&rsquo;s
What-If
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>, distracted users from the
de-biasing task.</p><div class=comment-holder><div class=comment><p>Of Silva&rsquo;s usability and effectiveness, <span class=citation-ref><a href=#Yan2020></a></span>claimed that it was
not dependent on practitioners' skills and therefore potentially more widely
applicable. However, their tone changed in <span class=citation-ref><a href=#Gu2021></a></span>(similar
authors).</p></div></div><h2 id=gender-bias>Gender Bias</h2><p><a id=Buolamwini2018Discussion></a><span class=citation-ref><a href=#Buolamwini2018></a></span>found two
prominent facial analysis benchmarks being overwhelmingly composed of
lighter-skinned faces. They then evaluated 3 commercial gender classification
systems on a new facial analysis dataset that is balanced by gender and skin
type. Darker-skinned females are the most misclassified (error-rates up to
34.7%), while the maximum error rate for lighter-skinned males was 0.8%.</p><p><span class=citation-ref><a href=#Tang2021></a></span>Image captioning datasets, e.g. COCO, contain gender bias
found in web corpora. They provide gender labels and split the COCO dataset to
expose biased models. Models that rely on contextual cues (e.g. motorcycles tend
to appear with men) fail more on the anti-stereotypical test data. They propose
a model that provides self-guidance on visual attention to encourage the model
to capture correct gender visual evidence.</p><figure><img src=/img/computer-science/bias-and-fairness/tang-2021.jpg alt="Tang2021's qualitative comparison of baseline and their proposed
	models" loading=lazy><figcaption>Tang2021's qualitative comparison of baseline and their proposed
models</figcaption></figure><div class=comment-holder><div class=comment><p><span class=citation-ref><a href=#Tang2021></a></span>propose a framework that teaches the model to look at the
correct part of the picture. That said,
<a href=#Buolamwini2018Discussion>facial recognition software has its
woes</a>
.</p></div></div><h2 id=recommendation-systems>Recommendation Systems</h2><p><span class=citation-ref><a href=#Yunqi2021></a></span>show that active users, who are the minority, enjoy much
higher recommendation quality than the majority inactive users. They propose a
re-ranking approach by adding a fairness constraint. They group users into two
groups in accordance with their activity levels, and aim to have the same
recommendation quality for each group. This approach reduces the unfairness
between advantaged and disadvantaged groups, and also improves overall
recommendation quality, at the cost of sacrificing the recommendation
performance of the advantaged group.</p><div class=comment-holder><div class=comment><p>There&rsquo;s a subtlety here. Although the active users are the minority, the
recommender considers them the majority as they are the ones providing a lot of
the training data.</p><p>Maybe recommendations based on collaborative filtering should also divide the
user base into similar cohorts, and learning can take place within these
cohorts? Hold up, doesn&rsquo;t that happen by definition? If I watch movie A and
someone else watched movies A and B, then the recommender can recommend B to me.
<span class=citation-ref><a href=#Yunqi2021></a></span>argues that my perception is inaccurate. \(\Delta\)</p></div></div><h2 id=policy>Policy</h2><p><span class=citation-ref><a href=#Mota2021></a></span>Primary source of school district revenue in the US is
public money. Existing school district boundaries promote financial segregation,
with highly-funded school districts surrounded by lesser-funded districts and
vice-versa. Authors propose the \({\rm F{\small AIR}\ P{\small ARTITIONING}}\)
problem to divide a given set of nodes (e.g. schools) into \(k\) partitions
(e.g. districts) such that the spatial inequality in a partition-level property
(e.g. funding) is minimized. They show that \({\rm F{\small AIR}\ P{\small
ARTITIONING}}\) is strongly NP-complete, and provide a reasonably effective
greedy algorithm. They further provide
<a href=https://redistricting.mpi-sws.org/ target=_blank rel=noopener>an interactive
website
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>for exploring the impact of school
redistricting.</p><div class=comment-holder><div class=comment><p>Surprised that the authors of <span class=citation-ref><a href=#Mota2021></a></span>are affiliated with non-US
institutions, but address a US problem. Fairness in the US school system is a
gnarly problem, e.g.
<a href=https://www.curiosities.dev/computer-science/bias-and-fairness/2021-07-04-rage-against-the-algo/#computers-can-solve-your-problem-you-may-not-like-the-answer-span-classcitation-refa-hrefbostonschoolsalgorithmaspan>Boston Public Schools trying for equity with an
algorithm</a>
.</p></div></div><h2 id=glossary>Glossary</h2><dl><dt id=GroupFairness style=font-weight:bolder>Group Fairness</dt><dd>The goal that based on the protected attribute, privileged and
unprivileged groups will be treated similarly.
<span class=citation-ref><a href=#></a></span></dd></dl><dl><dt id=ProtectedAttributes style=font-weight:bolder>Protected Attributes</dt><dd>Features that may not be used as the basis for decisions, e.g. gender
(and reassignment), race, religion, nationality, age, marital status,
disability, socioeconomic status, pregnancy, sexual orientation. In an unfair
ML system, the protected attribute divides the population into two groups
(privileged and unprivileged) that have differences in terms of receiving
benefits
<span class=citation-ref><a href=#EHRProtectedXtics></a></span><span class=citation-ref><a href=#OCWProtectedAttributes></a></span><span class=citation-ref><a href=#Chakraborty2021></a></span></dd></dl><h2 id=references>References</h2><ol><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=16><cite id=Yunqi2021>User-Oriented Fairness in Recommendation<i>.</i></cite>
Li, Yunqi; Chen, Hanxiong; Fu, Zuohui; Ge, Yingqiang; Zhang, Yongfeng.
The Web Conference, 2021.
Rutgers University.
<a href=https://doi.org/10.1145/3442381.3449866 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
<i>doi.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3Ayunqi+User-Oriented+Fairness+in+Recommendation+WWW+2021&btnG=" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
Apr 19, 2021.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 16 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=4><cite id=Tang2021>Mitigating Gender Bias in Captioning Systems<i>.</i></cite>
Tang, Ruixiang; Du, Mengnan; Li, Yuening; Liu, Zirui; Zou, Na; Hu, Xia.
The Web Conference, 2021.
Texas A&M University.
<a href=https://doi.org/10.1145/3442381.3449950 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
<i>doi.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3Atang+Mitigating+Gender+Bias+in+Captioning+Systems&btnG=" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
2021.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 4 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=0><cite id=Mota2021>Fair Partitioning of Public Resources: Redrawing District Boundary to Minimize Spatial Inequality in School Funding<i>.</i></cite>
Mota, Nuno; Mohammadi, Negar; Dey, Palash; Gummadi, Krishna P.; Chakraborty, Abhijnan.
The Web Conference, 2021.
Max Planck Institute for Software Systems; Tehran Institute for Advanced Studies; Indian Institute of Technology.
<a href=https://doi.org/10.1145/3442381.3450041 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
<i>doi.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3Amota+Fair+Partitioning+of+Public+Resources%3A+Redrawing+District+Boundary+to+Minimize+Spatial+Inequality+in+School+Funding&btnG=" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
2021.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 0 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=0><cite id=Gu2021>Understanding User Sense-making in Machine Learning Fairness Assessment Systems<i>.</i></cite>
Gu, Ziwei; Yan, Jing Nathan; Rzeszotarski, Jeffrey M..
The Web Conference, 2021.
Cornell University.
<a href=https://doi.org/10.1145/3442381.3450092 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
<i>doi.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3Agu+Understanding+User+Sense-making+in+Machine+Learning+Fairness+Assessment+Systems&btnG=" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
2021.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 0 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=2282><cite id=Buolamwini2018>Gender shades: Intersectional accuracy disparities in commercial gender classification.<i></i></cite>
Buolamwini, Joy; Timnit Gebru.
Machine Learning Research: Fairness, Accountability and Transparency, Vol 1, pp. 77-91, 2018.
Massachusetts Institute of Technology; Microsoft Research.
<a href=https://proceedings.mlr.press/v81/buolamwini18a.html target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=proceedings.mlr.press" loading=lazy aria-hidden=true>
<i>proceedings.mlr.press</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3A+Buolamwini+Gender+shades%3A+Intersectional+accuracy+disparities+in+commercial+gender+classification.&btnG=" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 2282 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=11><cite id=Yan2020>Silva: Interactively Assessing Machine Learning Fairness Using Causality<i>.</i></cite>
Jing Nathan Yan; Ziwei Gu; Hubert Lin; Jeffrey M. Rzeszotarski.
Human Factors in Computing Systems, 2020.
Cornell University.
<a href=https://doi.org/10.1145/3313831.3376447 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
<i>doi.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3Ayan+Silva%3A+Interactively+Assessing+Machine+Learning+Fairness+Using+Causality&btnG=" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 11 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-graduation-cap" cited-by-count=8><cite id=Chakraborty2021>Bias in Machine Learning Software: Why? How? What to Do?<i></i></cite>
Chakraborty, Joymallya; Suvodeep Majumder; Tim Menzies.
European Software Engineering Conference and Symposium on the Foundations of Software Engineering, Proceedings of the 29th, August 2021, pp. 429â€“440.
North Carolina State University.
<a href=https://doi.org/10.1145/3468264.3468537 target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
<i>doi.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C48&q=author%3Achakraborty+Bias+in+machine+learning+software%3A+why%3F+how%3F+what+to+do%3F&btnG=#d=gs_cit&u=%2Fscholar%3Fq%3Dinfo%3APeMEkQyQzV8J%3Ascholar.google.com%2F%26output%3Dcite%26scirp%3D0%26hl%3Den" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
<i>scholar.google.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<a href=https://github.com/joymallyac/Fair-SMOTE target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=github.com" loading=lazy aria-hidden=true>
<i>github.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-graduation-cap" aria-hidden=true></i>Cited 8 times as of Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count><cite id=OCWProtectedAttributes>Protected Attributes and 'Fairness through Unawareness' | Module 3: Pedagogical Framework for Addressing Ethical Challenges | Exploring Fairness in Machine Learning for International Development | MIT OpenCourseWare<i>.</i></cite>
<a href=https://ocw.mit.edu/resources/res-ec-001-exploring-fairness-in-machine-learning-for-international-development-spring-2020/module-three-framework/protected-attributes/ target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=ocw.mit.edu" loading=lazy aria-hidden=true>
<i>ocw.mit.edu</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Jan 23, 2022.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count><cite id=EHRProtectedXtics>Protected characteristics | Equality and Human Rights Commission<i>.</i></cite>
<a href=https://www.equalityhumanrights.com/en/equality-act/protected-characteristics target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=www.equalityhumanrights.com" loading=lazy aria-hidden=true>
<i>www.equalityhumanrights.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Jan 23, 2022.</div></li></ol></article><div style=font-size:smaller><aside id=tags-holder style="margin:0 0 2%">Tags:
<a href=/tags/ed-tech>#ed-tech</a>
<a href=/tags/gender-bias>#gender-bias</a>
<a href=/tags/privacy>#privacy</a>
<a href=/tags/re-ranking>#re-ranking</a>
<a href=/tags/recommendation-system>#recommendation-system</a></aside><aside id=authors-holder style="margin:0 0 2%">Cited Authors:
<a href=/cited-authors/Buolamwini-Joy>Buolamwini, Joy</a>
<a href=/cited-authors/Chakraborty-Abhijnan>Chakraborty, Abhijnan</a>
<a href=/cited-authors/Chakraborty-Joymallya>Chakraborty, Joymallya</a>
<a href=/cited-authors/Chen-Hanxiong>Chen, Hanxiong</a>
<a href=/cited-authors/Dey-Palash>Dey, Palash</a>
<a href=/cited-authors/Du-Mengnan>Du, Mengnan</a>
<a href=/cited-authors/Fu-Zuohui>Fu, Zuohui</a>
<a href=/cited-authors/Ge-Yingqiang>Ge, Yingqiang</a>
<a href=/cited-authors/Gebru-Timnit>Gebru, Timnit</a>
<a href=/cited-authors/Gu-Ziwei>Gu, Ziwei</a>
<a href=/cited-authors/Gummadi-Krishna-P.>Gummadi, Krishna P.</a>
<a href=/cited-authors/Hu-Xia>Hu, Xia</a>
<a href=/cited-authors/Li-Yuening>Li, Yuening</a>
<a href=/cited-authors/Li-Yunqi>Li, Yunqi</a>
<a href=/cited-authors/Lin-Hubert>Lin, Hubert</a>
<a href=/cited-authors/Liu-Zirui>Liu, Zirui</a>
<a href=/cited-authors/Majumder-Suvodeep>Majumder, Suvodeep</a>
<a href=/cited-authors/Menzies-Tim>Menzies, Tim</a>
<a href=/cited-authors/Mohammadi-Negar>Mohammadi, Negar</a>
<a href=/cited-authors/Mota-Nuno>Mota, Nuno</a>
<a href=/cited-authors/Rzeszotarski-Jeffrey-M.>Rzeszotarski, Jeffrey M.</a>
<a href=/cited-authors/Tang-Ruixiang>Tang, Ruixiang</a>
<a href=/cited-authors/Yan-Jing-Nathan>Yan, Jing Nathan</a>
<a href=/cited-authors/Zhang-Yongfeng>Zhang, Yongfeng</a>
<a href=/cited-authors/Zou-Na>Zou, Na</a></aside><aside id=publications-holder style="margin:0 0 2%">Cited Publications:
<a href=/publications/European-Software-Engineering-Conference-and-Symposium-on-the-Foundations-of-Software-Engineering>European Software Engineering Conference and Symposium on the Foundations of Software Engineering</a>
<a href=/publications/Human-Factors-in-Computing-Systems>Human Factors in Computing Systems</a>
<a href=/publications/Machine-Learning-Research:-Fairness-Accountability-and-Transparency>Machine Learning Research: Fairness, Accountability and Transparency</a>
<a href=/publications/The-Web-Conference>The Web Conference</a></aside><aside id=affiliations-holder style="margin:0 0 2%">Cited Authors' Affiliations:
<a href=/affiliations/Cornell-University>Cornell University</a>
<a href=/affiliations/Indian-Institute-of-Technology>Indian Institute of Technology</a>
<a href=/affiliations/Massachusetts-Institute-of-Technology>Massachusetts Institute of Technology</a>
<a href=/affiliations/Max-Planck-Institute-for-Software-Systems>Max Planck Institute for Software Systems</a>
<a href=/affiliations/Microsoft-Research>Microsoft Research</a>
<a href=/affiliations/North-Carolina-State-University>North Carolina State University</a>
<a href=/affiliations/Rutgers-University>Rutgers University</a>
<a href=/affiliations/Tehran-Institute-for-Advanced-Studies>Tehran Institute for Advanced Studies</a>
<a href=/affiliations/Texas-A&M-University>Texas A&M University</a></aside><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/doi.org style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=doi.org" loading=lazy aria-hidden=true>
doi.org</a>
<a href=/domains/github.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=github.com" loading=lazy aria-hidden=true>
github.com</a>
<a href=/domains/ocw.mit.edu style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=ocw.mit.edu" loading=lazy aria-hidden=true>
ocw.mit.edu</a>
<a href=/domains/proceedings.mlr.press style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=proceedings.mlr.press" loading=lazy aria-hidden=true>
proceedings.mlr.press</a>
<a href=/domains/scholar.google.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=scholar.google.com" loading=lazy aria-hidden=true>
scholar.google.com</a>
<a href=/domains/www.equalityhumanrights.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=www.equalityhumanrights.com" loading=lazy aria-hidden=true>
www.equalityhumanrights.com</a></aside></div></div><footer><a href=https://www.curiosities.dev/computer-science/bias-and-fairness/2021-03-03-on-the-dangers-of-stochastic-parrots/>&#171; On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>