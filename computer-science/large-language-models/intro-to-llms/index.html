<!doctype html><html lang=en><head><title>Introduction to LLMs | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="What is a Language Model? A language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \(\mathcal{V}\) of a set of tokens, then a language model \(p\) assigns each sequence of tokens \(x_1, &mldr;, x_L \in \mathcal{V} \) a probability. 
To assign meaningful probabilities to all sequences requires syntactic knowledge and world knowledge. Given \( \mathcal{V} = \{ \text{ate}, \text{ball}, \text{cheese}, \text{mouse}, \text{the} \} \):..."><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/large-language-models/>Large Language Models</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/>Introduction to LLMs</a></li></ul></nav><section><header><h1>Introduction to LLMs</h1><p class=meta>Dated Dec 14, 2023;
last modified on Thu, 14 Dec 2023</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#what-is-a-language-model>What is a Language Model?</a></li><li><a href=#references>References</a></li></ul></nav></aside><article id=main-article><h2 id=what-is-a-language-model>What is a Language Model?</h2><p>A <strong>language model (LM)</strong> is a probability distribution over sequences of
tokens. Suppose we have a <strong>vocabulary</strong> \(\mathcal{V}\) of a set of tokens,
then a language model \(p\) assigns each sequence of tokens \(x_1, &mldr;, x_L
\in \mathcal{V} \) a probability. <span class=citation-ref><a href=#CS324Intro></a></span></p><p>To assign meaningful probabilities to all sequences requires <strong>syntactic
knowledge</strong> and <strong>world knowledge.</strong> Given \( \mathcal{V} = \{ \text{ate},
\text{ball}, \text{cheese}, \text{mouse}, \text{the} \} \):</p><ul><li>The LM should assign a low value to \(p(\text{mouse}, \text{the}, \text{the},
\text{cheese}, \text{ate})\) because it&rsquo;s ungrammatical (syntactic
knowledge).</li><li>The LM should assign \( p(\text{the}, \text{mouse}, \text{ate}, \text{the},
\text{cheese}) \gt p(\text{the}, \text{cheese}, \text{ate}, \text{the},
\text{mouse}) \) because of world knowledge.</li></ul><p><span class=citation-ref><a href=#CS324Intro></a></span></p><p>We can also <strong>generate</strong> a sequence given an LM. The purest way to do this is to
sample a sequence \(x_{1:L}\) with probability equal to \(p(x_{1:L})\). In
practice, we don&rsquo;t sample directly from an LM because of limitations of real LMs
and the desire to get something closer to the &ldquo;best&rdquo; sequence and not just an
&ldquo;average&rdquo; sequence. <span class=citation-ref><a href=#CS324Intro></a></span></p><div class=comment-holder><div class="comment open-comment"><p>What are the said limitations of real-life LMs that prevent direct sampling?</p></div></div><p>Using the <strong>chain rule of probability:</strong></p><p>$$ p(x_{1:L}) = p(x_1) p(x_2 | x_1) p(x_3 | x1, x_2) &mldr; p(x_L | x_{1:L-1}) =
\Pi_{i=1}^{L} p(x_i | x_{1:i-1}) $$</p><p>&mldr; for example:</p><p>$$ p(\text{GSW}, \text{beats}, \text{Cavs}) = p(\text{GSW}) \cdot p(\text{beats}
| \text{GSW}) \cdot p(\text{Cavs} | \text{GSW}, \text{beats}) $$</p><p><span class=citation-ref><a href=#CS324Intro></a></span></p><p>An <strong>autoregressive language model</strong> is one where each conditional distribution
\(p(x_i | x_{1:i-1})\) can be computed efficiently. To generate a sequence
\(x_{1:L}\) from an autoregressive LM \(p\), we sample one token at a time,
i.e., for \(i = 1, &mldr;, L\):</p><p>$$ x_i \sim p \left(x_i | x_{1:i-1} \right)^{1/T} $$</p><p>&mldr; where \(T \ge 0\) is a <strong>temperature</strong> that controls randomness:</p><ul><li>\(T = 0\): deterministically choose the most probable \(x_i\).</li><li>\(T = 1\): sample &ldquo;normally&rdquo; from the pure LM.</li><li>\(T = \infty \): sample from a uniform distribution over \(\mathcal{V}\).</li></ul><p><span class=citation-ref><a href=#CS324Intro></a></span></p><p>Raising the probabilities to power \(1/T\) may make them not sum to \(1\),
but this is fixed by re-normalizing the distribution:</p><p>$$ p_T(x_i | x_{1:i-1}) \propto p \left( x_i | x_{1:i-1} \right) ^{1/T} $$</p><p>&mldr; also called the <strong>annealed</strong> conditional probability distribution. <span class=citation-ref><a href=#CS324Intro></a></span></p><p>We can perform <strong>conditional generation</strong> by specifying some prefix sequence
(called a <strong>prompt</strong>) and sampling the rest (called the <strong>completion</strong>).
Conditional generation allows LMs to solve a variety of tasks by simply changing
the prompt. <span class=citation-ref><a href=#CS324Intro></a></span></p><h2 id=references>References</h2><ol><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=CS324Intro>Introduction | CS324<i>.</i></cite>
Percy Liang; Tatsunori Hashimoto; Christopher RÃ©.
<a href=https://stanford-cs324.github.io/winter2022/lectures/introduction/ target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=stanford-cs324.github.io" loading=lazy aria-hidden=true>
<i>stanford-cs324.github.io</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
2022.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Dec 14, 2023.</div></li></ol></article><div style=font-size:smaller><aside id=authors-holder style="margin:0 0 2%">Cited Authors:
<a href=/cited-authors/Hashimoto-Tatsunori>Hashimoto, Tatsunori</a>
<a href=/cited-authors/Liang-Percy>Liang, Percy</a>
<a href=/cited-authors/R%c3%a9-Christopher>RÃ©, Christopher</a></aside><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/stanford-cs324.github.io style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=stanford-cs324.github.io" loading=lazy aria-hidden=true>
stanford-cs324.github.io</a></aside></div></div><footer><a href=https://www.curiosities.dev/computer-science/large-language-models/stochastic-parrots/>LLMs: Stochastic Parrots ðŸ¦œ and How (Not) to Use Them &#187;</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>