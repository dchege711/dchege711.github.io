<!doctype html><html lang=en><head><title>Using LLMs to Enhance My Capabilities | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="Sample Use Cases LLMs are increasingly here to stay despite the reservations  . How can I use them to enhance my capabilities?
Look out for the Gell-Man amnesia effect. You prompt the LLM on some subject you know well. You read the response and see the LLM has absolutely no understanding of either the facts or the issues. In any case, you read with exasperation or amusement the multiple errors in the response, and then ask it about something else, and read the response as if it&rsquo;s more accurate than the baloney you just read...."><meta property="og:title" content="Using LLMs to Enhance My Capabilities"><meta property="og:description" content="Sample Use Cases LLMs are increasingly here to stay despite the reservations  . How can I use them to enhance my capabilities?
Look out for the Gell-Man amnesia effect. You prompt the LLM on some subject you know well. You read the response and see the LLM has absolutely no understanding of either the facts or the issues. In any case, you read with exasperation or amusement the multiple errors in the response, and then ask it about something else, and read the response as if it&rsquo;s more accurate than the baloney you just read...."><meta property="og:type" content="website"><meta property="og:url" content="https://www.curiosities.dev/computer-science/large-language-models/using-llms/"><meta property="og:site_name" content="curiosities.dev"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><link rel=preload href=/css/vs.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/vs.min.css></noscript><script defer type=text/javascript src=/js/highlight.min.min.js onload=addURLHighlighter();></script><script defer>const hjlsURLRegex=/https?:\/\/[^\s<]+/g
const hjlsCitationRegex=/&lt;span class=&quot;citation-ref&quot;&gt;&lt;a href=&quot;(.*)&quot;&gt;&lt;\/a&gt;&lt;\/span&gt;/g
function addURLHighlighter(){hljs.addPlugin({"after:highlight":(result)=>{result.value=result.value.replaceAll(hjlsURLRegex,"<a href='$&' target='_blank'>$&</a>");console.log(result.value);result.value=result.value.replaceAll(hjlsCitationRegex,"<span class='citation-ref'><a href='$1'></a></span>");}});hljs.highlightAll();}</script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/large-language-models/>Large Language Models</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/large-language-models/using-llms/>Using LLMs to Enhance My Capabilities</a></li></ul></nav><section><header><h1>Using LLMs to Enhance My Capabilities</h1><p class=meta>Dated Dec 24, 2024;
last modified on Sun, 08 Jun 2025</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#sample-use-cases>Sample Use Cases</a></li><li><a href=#prompting-techniques>Prompting Techniques</a></li><li><a href=#references>References</a></li></ul></nav></aside><article id=main-article><h2 id=sample-use-cases>Sample Use Cases</h2><div class=priors-holder><div class=priors><p>LLMs are increasingly here to stay
<a href=https://www.curiosities.dev/computer-science/large-language-models/stochastic-parrots/>despite the reservations</a>
. How can I use
them to enhance my capabilities?</p><p>Look out for the Gell-Man amnesia effect. You prompt the LLM on some subject you
know well. You read the response and see the LLM has absolutely no understanding
of either the facts or the issues. In any case, you read with exasperation or
amusement the multiple errors in the response, and then ask it about something
else, and read the response as if it&rsquo;s more accurate than the baloney you just
read. <span class=citation-ref><a href=#GellMannAmnesia></a></span></p></div></div><p>Building complete applications, e.g., a trivia-like game with Python&rsquo;s Flask web
server. Makes it cheap to prototype in cases where the technology behind the
prototype matters much less than the content or problem being solved. <span class=citation-ref><a href=#Cartini2024></a></span></p><p>As a tutor for new technologies/framework. Although React might be new to <em>you</em>,
it&rsquo;s not new to other people. Something that probably exists in some tutorial
somewhere, e.g., blinking an LED on a Raspberry Pi. <span class=citation-ref><a href=#Cartini2024></a></span></p><p>For monotonous tasks, e.g., taking unstructured data and formatting it into a
structured format, citing web pages, etc. <span class=citation-ref><a href=#Cartini2024></a></span></p><p>Make every user a &ldquo;power user&rdquo;, e.g., hooking up an LLM into emacs to automate
transforming code snippets: <em>C-h C-h rewrite these #defines to a dictionary of
{keycode: string, &mldr;}</em>. <span class=citation-ref><a href=#Cartini2024></a></span></p><p>As an API reference. <em>Write a latex command <code>\red{}</code> that turns the text red.</em>
<em>lldb &ldquo;i r&rdquo; equivalent</em>. <em>What does this do: <code>find . -name '*txt' -exec bash -c ' mv $0 ${0/\ZZ_/}' {} \;</code></em>. <span class=citation-ref><a href=#Cartini2024></a></span></p><p>To search for things that are hard to find. For traditional search engines, it
seems like you&rsquo;re trying to use keywords that the answer will have and not the
question. <em>So I know that <code>+</code> corresponds to <code>__add__</code> but what is <code>~</code>?</em> <span class=citation-ref><a href=#Cartini2024></a></span></p><p>To solve one-off tasks. These are disposable programs where cleanliness doesn&rsquo;t
matter. <em>I have a file with a JSON object on each line (like jsonl). Each line
has a key called &ldquo;text&rdquo;, along with some other I don&rsquo;t care about. Can you write
a function to extract the string on each line in python?</em> <em>In bash every second
write the memory usage and average CPU usage to a file.</em> <span class=citation-ref><a href=#Cartini2024></a></span></p><p>To explain things. While LLMs are not as knowledgeable as the best person in the
world, there are thousands/millions of people who know answers to the questions
that I have, and so the LLMs probably have the answer too. <em>What does this mean
60mW/sr @ 100mA</em> <span class=citation-ref><a href=#Cartini2024></a></span>LLMs are useful in grounding you in
the rough shape and jargon of a field; use them as a starting point for building
an accurate mental model of how the field works; treat LLMs as just one
unreliable source of information. <span class=citation-ref><a href=#CartiniHN2024></a></span><span class=tag-ref><a href=/tags/knowing>#knowing</a></span></p><div class=comment-holder><div class=comment><p>Advice in
<a href=https://www.curiosities.dev/knowing/2020-05-02-on-learning/>On Learning</a>
applies
here.</p></div></div><p>To solve tasks with known solutions. <em>Can you help me convert this python
program into a C program that does the same thing but is faster?</em> <em>Rewrite this
function in rust.</em> <em><code>mp.Pool</code> parallelize this with 128 jobs across the size of
the file.</em> <span class=citation-ref><a href=#Cartini2024></a></span></p><div class=comment-holder><div class=comment><p>Skill as a code reviewer comes in handy. Can you tell when the LLM is
hallucinating or inaccurate? If I don&rsquo;t know Rust, how can I be confident that
the program is correct?</p></div></div><p>To fix common errors. <em>Ask LLM &ldquo;How do I fix this error? [error]"; apply
step-by-step solution as suggested by LLM; if it does not work, say &ldquo;that didn&rsquo;t
work&rdquo;.</em> <span class=citation-ref><a href=#Cartini2024></a></span></p><p>Whenever the AI gives a fix but you don&rsquo;t understand why, ask, e.g., <em>can you
explain why that change solves the problem?</em> That way, you learn for next time,
and can double check that the AI&rsquo;s reasoning is sound. <span class=citation-ref><a href=#Osmani2025></a></span></p><div class=comment-holder><div class="comment open-comment"><p>IIRC, chain of thought explanations are inferred after the fact. How much weight
should I give them given that they&rsquo;re, in a sense, predetermined? Maybe they
give enough pointers for me to verify elsewhere on their plausibility/accuracy?</p></div></div><p>Converse with the rubber duck, e.g., <em>I will explain what this function is
doing: [your explanation]. Given that, is my reasoning correct and does it
reveal where the bug is?</em> <span class=citation-ref><a href=#Osmani2025></a></span></p><p>To create structured output while also reasoning, try a 2-step approach, e.g.,</p><ul><li>Prompt 1: <em>Do {the thing described in detail}. End with a clear summary of
your answer that includes {things you need for JSON}.</em></li><li>Prompt 2: *A previous agent said {content}. Structure as JSON according to
{schema}. Use {tool} to validate the JSON.</li></ul><p><span class=citation-ref><a href=#OsmaniHN2025></a></span></p><h2 id=prompting-techniques>Prompting Techniques</h2><div class=comment-holder><div class="comment open-comment"><p>I don&rsquo;t like the term &ldquo;prompt engineering&rdquo; because there&rsquo;s not a lot of science
and reproducibility behind it. Evals are the standard for evaluating prompts,
but a lot of the advice out there does not come with evidence of evals.</p></div></div><p>Provide context, e.g., <em>I have a Node.js function using Express and Mongoose
that should fetch a user by ID, but it throws a TypeError. Here&rsquo;s the code and
error&mldr;</em> <span class=citation-ref><a href=#Osmani2025></a></span>VS Code has an improved UX where the LLM can
search the codebase&rsquo;s index to select potentially relevant context to include in
the prompt (
<a href=https://www.curiosities.dev/computer-science/large-language-models/writing-code-with-llms/ target=_blank rel=noopener>notes
<i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>).</p><div class=comment-holder><div class=comment><p>Seems like the goal is to provide context that the LLM might not infer by itself,
e.g., the fact that we&rsquo;re using Express and Mongoose.</p></div></div><div class=comment-holder><div class=comment><p>Maybe I should think of it as trying to navigate a massive probability space?
Given this much context, what is the probability of generating a
useful/plausible response? We don&rsquo;t want to sample from the universe of where
everything is possible.</p></div></div><p>Be specific about your goals, e.g., <em>How can I improve the performance of this
sorting function for 10k items? {{ code }}</em>. <span class=citation-ref><a href=#Osmani2025></a></span></p><p>Break down complex tasks into smaller chunks and iterate, e.g., <em>Outline a plan
to add a search feature that filters a list of products by name in my React app.
The products are fetched from an API.</em> Refine the plan. <em>Okay, implement step 1:
create a <code>SearchBar</code> component with an input that updates a <code>searchQuery</code>
state.</em> And so forth, iterating where need be. <span class=citation-ref><a href=#Osmani2025></a></span></p><p>Include examples of expected behavior, e.g., <em>Given the array [3, 1, 4], this
function should return [1, 3, 4].</em> Conventionally referred to as <strong>few-shot
prompting</strong>. <span class=citation-ref><a href=#Osmani2025></a></span></p><p>Leverage roles or personas, e.g., <em>You are a JavaScript performance expert.
Optimize the following function for execution speed.</em> <span class=citation-ref><a href=#Osmani2025></a></span>Role prompting (e.g., <em>You are an expert doctor, help me with this rash I have
all over.</em>) doesn&rsquo;t trump giving the right context (e.g., <em>pt presents w
bilateral erythema, need diff dx</em>). Others have noted that telling the LLM that
someone else wrote something and you need their help assessing the quality of it
makes the LLM more cutthroat and direct instead of sycophantic and deferential.
<span class=citation-ref><a href=#OsmaniHN2025></a></span></p><p>Iterate and refine the conversation, e.g., <em>That solution uses recursion, but
I&rsquo;d prefer an iterative approach &ndash; can you try again without recursion?</em>
<em>Actually, in our app we use a custom hook <code>useProducts()</code> that already handles
fetching. Please refactor to use <code>useProducts</code> instead of directly calling
<code>fetch</code>.</em> <span class=citation-ref><a href=#Osmani2025></a></span></p><p>Keep a high quality bar because the LLM will anchor on the snippets and context
provided. If coding fast and loose to get something working, might want to
disable LLM completions because &ldquo;garbage in, garbage out&rdquo; applies. <span class=citation-ref><a href=#CopilotPromptEngineering></a></span></p><h2 id=references>References</h2><ol><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=Cartini2024>How I Use 'AI'<i>.</i></cite>
Nicholas Cartini.
<a href=https://nicholas.carlini.com/writing/2024/how-i-use-ai.html target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=nicholas.carlini.com" loading=lazy aria-hidden=true width=16 height=16>
<i>nicholas.carlini.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
Aug 1, 2024.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Dec 24, 2024.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=CartiniHN2024>How I Use 'AI' | Hacker News<i>.</i></cite>
<a href="https://news.ycombinator.com/item?id=41150317" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=news.ycombinator.com" loading=lazy aria-hidden=true width=16 height=16>
<i>news.ycombinator.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Dec 24, 2024.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=Osmani2025>The Prompt Engineering Playbook for Programmers<i>.</i></cite>
Addy Osmani.
<a href=https://addyo.substack.com/p/the-prompt-engineering-playbook-for target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=addyo.substack.com" loading=lazy aria-hidden=true width=16 height=16>
<i>addyo.substack.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
May 27, 2025.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Jun 4, 2025.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=OsmaniHN2025>Prompt engineering playbook for programmers | Hacker News<i>.</i></cite>
<a href="https://news.ycombinator.com/item?id=44182188" target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=news.ycombinator.com" loading=lazy aria-hidden=true width=16 height=16>
<i>news.ycombinator.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Jun 4, 2025.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=GellMannAmnesia>Gell-Mann amnesia effect - Wikipedia<i>.</i></cite>
<a href=https://en.wikipedia.org/wiki/Gell-Mann_amnesia_effect target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=en.wikipedia.org" loading=lazy aria-hidden=true width=16 height=16>
<i>en.wikipedia.org</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Jun 8, 2025.</div></li><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=CopilotPromptEngineering>Prompt engineering for Copilot Chat<i>.</i></cite>
<a href=https://code.visualstudio.com/docs/copilot/chat/prompt-crafting target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=code.visualstudio.com" loading=lazy aria-hidden=true width=16 height=16>
<i>code.visualstudio.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Jun 3, 2025.</div></li></ol></article><div style=font-size:smaller><aside id=tags-holder style="margin:0 0 2%">Tags:
<a href=/tags/knowing>#knowing</a></aside><aside id=authors-holder style="margin:0 0 2%">Cited Authors:
<a href=/cited-authors/Cartini-Nicholas>Cartini, Nicholas</a>
<a href=/cited-authors/Osmani-Addy>Osmani, Addy</a></aside><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/addyo.substack.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=addyo.substack.com" loading=lazy aria-hidden=true width=16 height=16>
addyo.substack.com</a>
<a href=/domains/code.visualstudio.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=code.visualstudio.com" loading=lazy aria-hidden=true width=16 height=16>
code.visualstudio.com</a>
<a href=/domains/en.wikipedia.org style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=en.wikipedia.org" loading=lazy aria-hidden=true width=16 height=16>
en.wikipedia.org</a>
<a href=/domains/news.ycombinator.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=news.ycombinator.com" loading=lazy aria-hidden=true width=16 height=16>
news.ycombinator.com</a>
<a href=/domains/nicholas.carlini.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=nicholas.carlini.com" loading=lazy aria-hidden=true width=16 height=16>
nicholas.carlini.com</a></aside></div></div><footer><a href=https://www.curiosities.dev/computer-science/large-language-models/writing-code-with-llms/>&#171; Copilot in VS Code</a>
<a href=https://www.curiosities.dev/computer-science/large-language-models/popping-bubbles/>Popping Bubbles Game for Computer-Use Models &#187;</a></footer></section></div><footer><a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>