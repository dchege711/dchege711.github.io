<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large Language Models on Chege's Blog</title><link>https://www.curiosities.dev/computer-science/large-language-models/</link><description>Recent content in Large Language Models on Chege's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 14 Dec 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/computer-science/large-language-models/index.xml" rel="self" type="application/rss+xml"/><item><title>Introduction to LLMs</title><link>https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/intro-to-llms/</guid><description>What is a Language Model? A language model (LM) is a probability distribution over sequences of tokens. Suppose we have a vocabulary \(\mathcal{V}\) of a set of tokens, then a language model \(p\) assigns each sequence of tokens \(x_1, &amp;hellip;, x_L \in \mathcal{V} \) a probability.
To assign meaningful probabilities to all sequences requires syntactic knowledge and world knowledge. Given \( \mathcal{V} = \{ \text{ate}, \text{ball}, \text{cheese}, \text{mouse}, \text{the} \} \):</description></item><item><title>LLMs: Stochastic Parrots ðŸ¦œ and How (Not) to Use Them</title><link>https://www.curiosities.dev/computer-science/large-language-models/stochastic-parrots/</link><pubDate>Wed, 03 Mar 2021 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/stochastic-parrots/</guid><description>was written in a period when NLP practitioners are producing bigger (# of parameters; size of training data) language models (LMs), and pushing the top scores on benchmarks. The paper itself was controversial because it led to Gebru being fired from Google, following disagreements with her managers on conditions (withdraw, or remove Google-affiliated authors) for publishing the paper.
A lot changed since mid-2021, when I initially wrote this page.</description></item></channel></rss>