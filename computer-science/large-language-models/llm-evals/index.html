<!doctype html><html lang=en><head><title>LLM Evals | curiosities.dev</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=generator content="Hugo (https://gohugo.io/)"><meta name=description content="Notable Benchmarks Some notable benchmarks in language modeling:
 MMLU: 57 tasks spanning elementary math, US history, computer science, law, and more. EleutherAI Eval: Unified framework to test models via zero/few-shot settings on 200 tasks from various evals, including MMLU. HELM: Evaluates LLMs across domains; tasks include Q&A, information retrieval, summarization, text classification, etc. AlpacaEval: Measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model...."><meta property="og:title" content="LLM Evals"><meta property="og:description" content="Notable Benchmarks Some notable benchmarks in language modeling:
 MMLU: 57 tasks spanning elementary math, US history, computer science, law, and more. EleutherAI Eval: Unified framework to test models via zero/few-shot settings on 200 tasks from various evals, including MMLU. HELM: Evaluates LLMs across domains; tasks include Q&A, information retrieval, summarization, text classification, etc. AlpacaEval: Measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model...."><meta property="og:type" content="website"><meta property="og:url" content="https://www.curiosities.dev/computer-science/large-language-models/llm-evals/"><meta property="og:site_name" content="curiosities.dev"><link rel=stylesheet type=text/css href=/css/main.min.css><link rel=preload href=/css/all_font_awesome_v5.9.min.min.css as=style onload="this.onload=null;this.rel='stylesheet'"><noscript><link rel=stylesheet href=/css/all_font_awesome_v5.9.min.min.css></noscript><link rel="shortcut icon" href=/img/favicon_io/favicon.ico><link rel=apple-touch-icon sizes=180x180 href=/img/favicon_io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/img/favicon_io/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/img/favicon_io/favicon-16x16.png><script async type=text/javascript src=/js/OrganizeCitations.min.js></script><script async type=text/javascript src=/js/HighlightAnchor.min.js></script><script async type=text/javascript src=/js/SummaryPageUtils.min.js></script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body><div class=container id=main_div><form action=/search method=get id=globalSearchForm><input type=text id=q name=q title="Search Query">
<input type=submit id=submitButton value=Search></form><nav aria-label=Breadcrumb class=breadcrumb><ul><li><a href=https://www.curiosities.dev/>Home</a></li><li><a href=https://www.curiosities.dev/computer-science/>Computer Science & Software Engineering</a></li><li><a href=https://www.curiosities.dev/computer-science/large-language-models/>Large Language Models</a></li><li class=active><a href=https://www.curiosities.dev/computer-science/large-language-models/llm-evals/>LLM Evals</a></li></ul></nav><section><header><h1>LLM Evals</h1><p class=meta>Dated Apr 6, 2025;
last modified on Sun, 06 Apr 2025</p></header><div id=toc-then-article><aside id=toc><nav id=TableOfContents><ul><li><a href=#notable-benchmarks>Notable Benchmarks</a></li><li><a href=#commonly-used-metrics>Commonly Used Metrics</a><ul><li><a href=#bilingual-evaluation-understudy-bleu>Bilingual Evaluation Understudy (BLEU)</a></li><li><a href=#recall-oriented-understudy-for-gisting-evaluation-rogue>Recall-Oriented Understudy for Gisting Evaluation (ROGUE)</a></li><li><a href=#bertscore>BERTScore</a></li><li><a href=#moverscore>MoverScore</a></li></ul></li><li><a href=#pitfalls-to-using-conventional-benchmarks-and-metrics>Pitfalls to Using Conventional Benchmarks and Metrics</a></li><li><a href=#using-a-strong-llm-as-a-reference-free-metric>Using a Strong LLM as a Reference-Free Metric</a></li><li><a href=#how-to-apply-evals>How to Apply Evals</a></li><li><a href=#references>References</a></li></ul></nav></aside><article id=main-article><h2 id=notable-benchmarks>Notable Benchmarks</h2><p>Some notable benchmarks in language modeling:</p><ul><li><strong>MMLU:</strong> 57 tasks spanning elementary math, US history, computer science,
law, and more.</li><li><strong>EleutherAI Eval:</strong> Unified framework to test models via zero/few-shot
settings on 200 tasks from various evals, including MMLU.</li><li><strong>HELM:</strong> Evaluates LLMs across domains; tasks include Q&A, information
retrieval, summarization, text classification, etc.</li><li><strong>AlpacaEval:</strong> Measures how often a strong LLM (e.g., GPT-4) prefers the
output of one model over a reference model.</li></ul><p><span class=citation-ref><a href=#Yan2023></a></span></p><h2 id=commonly-used-metrics>Commonly Used Metrics</h2><h3 id=bilingual-evaluation-understudy-bleu>Bilingual Evaluation Understudy (BLEU)</h3><p>\(\text{BLEU}\) counts the number of n-grams in the generated output that also
show up in the reference, and then divides it by the total number of words in
the output. To counter unigrams of common words from dominating, a brevity
penalty is added to penalize excessively short sentences. <span class=citation-ref><a href=#Yan2023></a></span></p><p>\(\text{BLEU}\) is predominantly used in machine translation. <span class=citation-ref><a href=#Yan2023></a></span></p><h3 id=recall-oriented-understudy-for-gisting-evaluation-rogue>Recall-Oriented Understudy for Gisting Evaluation (ROGUE)</h3><p>\(\text{ROGUE}\) counts the number of words in the reference that also occur
in the output. It has several variants:</p><ul><li>\(\text{ROGUE-N}\) also counts the number of matching n-grams between the
output and the reference.</li><li>\(\text{ROUGE-L}\) counts the longest common subsequence between the output
and the reference.</li><li>\(\text{ROUGE-S}\) measures the skip-bigram (pairs of words that maintain
their relative order regardless of words sandwiched in between) between the
output and the reference.</li></ul><p><span class=citation-ref><a href=#Yan2023></a></span></p><h3 id=bertscore>BERTScore</h3><p>\(\text{BERTScore}\) uses contextualized embeddings and has 3 components:</p><ul><li>Recall: Average cosine similarity between each token in the reference and its
closest match in the output.</li><li>Precision: Average cosine similarity between each token in the output and its
nearest match in the reference.</li><li>F1: Harmonic mean of recall and precision.</li></ul><p><span class=citation-ref><a href=#Yan2023></a></span></p><p>Unlike \(\text{BLEU}\) and \(\text{ROGUE}\) which rely on exact matches,
\(\text{BERTScore}\) can account for synonyms and paraphrasing. It has better
correlation for image captioning and machine translation. <span class=citation-ref><a href=#Yan2023></a></span></p><h3 id=moverscore>MoverScore</h3><p>\(\text{MoverScore}\) is like \(\text{BERTScore}\), but allows for
many-to-one matching (soft-alignment) instead of one-to-one matching (hard
alignment). \(\text{MoverScore}\) measures the distance that words would have
to move to convert one sequence to another. <span class=citation-ref><a href=#Yan2023></a></span></p><h2 id=pitfalls-to-using-conventional-benchmarks-and-metrics>Pitfalls to Using Conventional Benchmarks and Metrics</h2><p><strong>Poor correlation between these metrics and human judgments.</strong>
\(\text{BLEU}\) and \(\text{ROUGE}\) have negative correlations w/ how
humans evaluate fluency, and low correlation with tasks that require creativity
and diversity. <span class=citation-ref><a href=#Yan2023></a></span></p><p><strong>Poor adaptability.</strong> Exact match metrics like \(\text{BLEU}\) and
\(\text{ROGUE}\) are a poor fit for abstract summarization and dialog where an
output can have zero n-gram overlap with the reference but yet be a good
response. <span class=citation-ref><a href=#Yan2023></a></span></p><p><strong>Poor reproducibility.</strong> Attributed to variations in human judgement
collection, metric parameter settings, incorrect eval implementations, different
prompts of the same example across eval providers. <span class=citation-ref><a href=#Yan2023></a></span></p><h2 id=using-a-strong-llm-as-a-reference-free-metric>Using a Strong LLM as a Reference-Free Metric</h2><p>\(\text{G-EVAL}\) is a framework that applies LLMs to evaluate LLM outputs.
Give the LLM a Task Introduction and Evaluation Criteria, and then ask it to
generate a Chain-of-Thought (CoT) of detailed Evaluation Steps. Concatenate the
prompt, CoT, input and output, and ask the LLM to output a score. Use the
probability-weighted summation of the output scores as the final score. <span class=citation-ref><a href=#Yan2023></a></span></p><figure><img src=/img/computer-science/llms/geval.jpg alt="Overall framework of G-EVAL."><figcaption><p>Overall framework of G-EVAL.</p></figcaption></figure><p>GPT-4 as an evaluator had a high Spearman correlation with human judgements
(\(0.514\)), outperforming all previous methods. LLM-based automated evals
could be a cost-effective and reasonable alternative to human evals. <span class=citation-ref><a href=#Yan2023></a></span></p><h2 id=how-to-apply-evals>How to Apply Evals</h2><p>Eval Driven Development. Collect a set of task-specific evals (i.e., prompt,
context, expected outputs). These evals then guide prompt engineering, model
selection, fine-tuning, and so on. <span class=citation-ref><a href=#Yan2023></a></span></p><p>Compared to human judgements, LLM judgements tend to be less noisy (more
systematic bias) but more biased. Mitigate these biases:</p><ul><li><strong>Position bias:</strong> LLMs tend to favor the response in the first position.
Evaluate the same pair of responses twice while swapping their order. Only
mark wins when the same response is preferred in both orders.</li><li><strong>Verbosity bias:</strong> LLMs tend to favor wordier responses. Ensure that
comparison responses are similar in length.</li><li><strong>Self-enhancement bias:</strong> Don&rsquo;t use the same LLM for evaluation tasks.</li></ul><h2 id=references>References</h2><ol><li><div class=citation citation-icon-class="fas fa-fw fa-globe" cited-by-count is-main><cite id=Yan2023>Patterns for Building LLM-based Systems & Products<i>.</i></cite>
<a href=https://eugeneyan.com/writing/llm-patterns/ target=_blank rel=noopener><img src="https://www.google.com/s2/favicons?domain=eugeneyan.com" loading=lazy aria-hidden=true width=16 height=16>
<i>eugeneyan.com</i> <i class="fas fa-fw fa-external-link-alt" aria-hidden=true></i></a>.
<i class="fas fa-fw fa-globe" aria-hidden=true></i>Accessed Apr 6, 2025.</div></li></ol></article><div style=font-size:smaller><aside id=domains-holder style="margin:0 0 2%">Cited Domains:
<a href=/domains/eugeneyan.com style="margin:0 2px"><img src="https://www.google.com/s2/favicons?domain=eugeneyan.com" loading=lazy aria-hidden=true width=16 height=16>
eugeneyan.com</a></aside></div></div><footer><a href=https://www.curiosities.dev/computer-science/large-language-models/ux-for-llms/>&#171; UX for LLMs</a></footer></section></div><footer><a href=mailto:d.chege711@gmail.com>Email</a>
<a href=/about>About</a>
<a href=/search>Search</a></footer></body></html>