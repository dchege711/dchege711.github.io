---
layout: notebook
created: 2018-06-26
title: Applied Machine Learning
modified: 2019-02-15
subcategory: All Stories Tech
---

<div>
 <p>
  <a href="http://www.basilica.ai/blog/the-unreasonable-effectiveness-of-deep-feature-extraction/">
   <span>
    (02/2019: Basilica) The Unreasonable Effectiveness
  of Deep Feature Extraction
   </span>
  </a>
 </p>
 <p>
  Embedding:
  <span>
   anything which maps an object in one space to
  point in a vector space, e.g. Word2Vec . Usually, either that the second
  space is lower-dimensional, or that objects which are similar by some
  important metric are close together in the second space.
  </span>
 </p>
 <p>
  <a href="https://arxiv.org/pdf/1403.6382.pdf">
   Razavian et al: CNN Features
  Off-the-Shelf
  </a>
 </p>
 <ul>
  <li>
   <span>
    Took a deep neural network
       (OverFeat), pretrained it on millions of images (ImageNet), and then
       repurposed it as a feature extractor.
   </span>
  </li>
  <li>
   <span>
    Fed the extractor with the
       Oxford Flowers dataset, and fed resulting features into an SVM
   </span>
  </li>
  <ul>
   <li>
    <span>
     ImageNet
        has general pictures like dogs, strawberries, etc.
    </span>
   </li>
   <li>
    <span>
     Oxford
        Flowers has fine-grained distinctions between different flower types
    </span>
   </li>
  </ul>
  <li>
   <span>
    They achieved better results
       than top-performing baselines!
   </span>
  </li>
 </ul>
 <p>
  <a href="https://arxiv.org/pdf/1712.00409.pdf">
   Baidu Research
  </a>
  <span>
   found that after a certain point, it takes a lot of
  data to meaningfully improve your results (the error rate improves extremely
  sub-linearly)
  </span>
 </p>
 <p>
  Implication:
  <span>
   if you take this huge network pretrained on a
  billion images, repurpose it as a feature extractor, and use it to train a
  simple linear model on your dataset - it's almost as good as finetuning the
  whole thing.
   Crucially, it
  significantly outperforms training a neural network directly on your dataset!
  </span>
 </p>
 <p>
  Why
  are embeddings poised to become a big deal?
 </p>
 <ul>
  <li>
   <span>
    Embeddings have short
       train-test cycles.
   </span>
  </li>
  <li>
   <span>
    You can change the model
       producing the embedding, or change the model consuming the embedding,
       w/o worrying about the other one.
   </span>
  </li>
  <li>
   <span>
    Embeddings provide an easy
       way to use deep learning to improve existing models (legacy
       infrastructure)
   </span>
  </li>
 </ul>
 <p>
  What
  still needs to happen?
 </p>
 <ul>
  <li>
   <span>
    Big Tech currently produces
       the best pretrained networks. Today's good PR is tomorrow's competitive
       advantage?
   </span>
  </li>
  <li>
   <span>
    Running state of the art
       pretrained networks with reasonable latency requires specialized
       hardware.
   </span>
  </li>
  <li>
   <span>
    There's no good pretrained
       network for things like product images or typo-filled support requests.
   </span>
  </li>
 </ul>
</div>
<div>
 <p>
  <a href="https://www.microsoft.com/en-us/research/blog/what-are-the-biases-in-my-data/">
   <span>
    (02/2019: Microsoft Research) What are the biases in
  my data?
   </span>
  </a>
 </p>
 <p>
  Impact:
  <span>
   word embeddings might be naively used to match
  resumes to jobs, a methodical way of checking training data for biases
  </span>
 </p>
 <p>
  Method:
  <span>
   use the geometry of the word embeddings
  (Word2Vec,
   FastText, GloVe) to find
  parallels between clusters of lower-case words and first names.
  </span>
 </p>
 <p>
  We recover gender
  groups but also a number of other biases including religious, ethnic, age,
  and racial groups emerge.
 </p>
 <p>
  <img alt="https://www.microsoft.com/en-us/research/uploads/prod/2019/02/Table-1_cluster-of-first-names.png" src="applied_ml_ai_files/11573D37-3D88-5945-8B5C-EE8806A84552.png"/>
 </p>
 <p>
  Ask an embedding:
 </p>
 <ul>
  <li>
   <span>
    Man -&gt;
       woman; doctor -&gt; ? Nurse!
   </span>
  </li>
  <li>
   <span>
    Man -&gt;
       woman; computer programmer -&gt; ? Homemaker!
   </span>
  </li>
  <li>
   <span>
    Kosher,
       hummus, and bagel? [Israeli name cluster]!
   </span>
  </li>
 </ul>
 <p>
  It gets worse&hellip;
 </p>
 <p>
  <img alt="https://www.microsoft.com/en-us/research/uploads/prod/2019/02/table-2_most-offensive-to-crowd-sourcing.png" src="applied_ml_ai_files/5A4A854A-77D3-8C4B-B4D6-4673999532D0.png"/>
 </p>
</div>
<div>
 <p>
  <a href="https://eng.uber.com/forecasting-introduction/">
   <span>
    (09/2018: Uber) Forecasting at Uber - An Introduction
   </span>
  </a>
 </p>
 <p>
  <span>
   Motivations for forecasting:
  </span>
 </p>
 <ul>
  <li>
   <span>
    Predict
       user supply and demand in a spatio-temporal fine granular fashion to
       direct driver-partners to high demand areas before they arise
   </span>
  </li>
  <li>
   <span>
    Find the
       sweet spot between hardware under-provisioning (erodes user trust) and
       over-provisioning (costly)
   </span>
  </li>
  <li>
   <span>
    Understand
       the marginal effectiveness of different media channels while controlling
       for trends, seasonality, and other dynamics (e.g., competition or
       pricing)
   </span>
  </li>
 </ul>
 <p>
  <span>
   What makes forecasting (at Uber) challenging?
  </span>
 </p>
 <ul>
  <li>
   <span>
    Uber has
       many actors of diverse behavior and interests, physical constraints, and
       unpredictability.
   </span>
  </li>
  <li>
   <span>
    Physical
       constraints, like geographic distance and road throughput move
       forecasting from the temporal to spatio-temporal domains.
   </span>
  </li>
 </ul>
 <p>
  <span>
   What are the right tools for the job?
  </span>
 </p>
 <ul>
  <li>
   <span>
    Model-based forecasting
   </span>
  </li>
  <ul>
   <li>
    <span>
     Strongest
        choice when the underlying mechanism of the problem is known
    </span>
   </li>
  </ul>
  <li>
   <span>
    Statistical Model
   </span>
  </li>
  <ul>
   <li>
    <span>
     When the
        underlying mechanisms are not known or are too complicated, e.g., the
        stock market, or not fully known, e.g., retail sales
    </span>
   </li>
  </ul>
  <li>
   <span>
    Machine Learning Models
   </span>
  </li>
  <ul>
   <li>
    <span>
     When
        interpretability is not a requirement
    </span>
   </li>
  </ul>
 </ul>
</div>
<div>
 <p>
  <a href="https://ai.googleblog.com/2019/02/real-time-continuous-transcription-with.html">
   <span>
    (02/2019: Google AI) Real-time Continuous
  Transcription with Live Transcribe
   </span>
  </a>
 </p>
 <p>
  We
  implemented an on-device neural network-based speech detector, built on our
  previous work with AudioSet. This network is an image-like model, similar to
  our published VGGish model, which detects speech and automatically manages
  network connections to the cloud ASR engine, minimizing data usage over long
  periods of use.
 </p>
 <p>
  Accessibility
  UX Decisions:
 </p>
 <ul>
  <li>
   <span>
    Smartphone form factor
       because of the sheer ubiquity of these devices and the increasing
       capabilities they have.
   </span>
  </li>
  <li>
   <span>
    Loudness and noise indicator
       of their current environment.
   </span>
  </li>
  <ul>
   <li>
    <span>
     The inner
        brighter circle, indicating the noise floor, tells a deaf user how
        audibly noisy the current environment is.
    </span>
   </li>
   <li>
    <span>
     The outer
        circle shows how well the speaker&rsquo;s voice is received.
    </span>
   </li>
  </ul>
  <li>
   <span>
    Encoding confidence levels
       of the transcription (e.g. yellow = high, green = medium, blue = low,
       white = awaiting context) was found to be distracting to the user
       without providing conversational value.
   </span>
  </li>
 </ul>
</div>
<div>
 <p>
  <a href="https://code.fb.com/connectivity/electrical-grid-mapping/">
   <span>
    (01/2019: FB Research) A New Predictive Model for
  More Accurate Electrical Grid Mapping
   </span>
  </a>
 </p>
 <p>
  A
  Computer Vision approach did not work because:
 </p>
 <ul>
  <li>
   <span>
    The shape
       and orientation of poles and structures are so diverse in form that even
       large image training sets were insufficient to generate high-quality
       labeled examples.
   </span>
  </li>
  <li>
   <span>
    Vegetation,
       shadows, and nearby similar infrastructure made it hard to correctly
       identify MV grid lines.
   </span>
  </li>
 </ul>
 <p>
  We
  decided to instead try a predictive modeling approach, using indicators of
  electrification, mainly based on nighttime radiance:
 </p>
 <ul>
  <li>
   <span>
    Assumptions:
   </span>
  </li>
  <ul>
   <li>
    <span>
     All
        settlements that generate detectable lighting are on the grid
    </span>
   </li>
   <li>
    <span>
     All
        settlements that do not are off the grid.
    </span>
   </li>
  </ul>
  <li>
   <span>
    Mitigated assumptions by
       using local data,
   </span>
   <span>
    e.g. electrification status for all schools in
       Nigeria.
   </span>
  </li>
 </ul>
 <p>
  We
  corrected for reflected starlight and moonlight, clouds, wildfires,
  lightning, oil well flares, fishing boats, bonfires, and reflected city
  lights using NASA's monthly high-res images. The points that remained were
  assumed to be settlements with a connection to grid infrastructure, in the
  form of an MV distribution line.
 </p>
 <p>
  <a href="https://github.com/facebookresearch/many-to-many-dijkstra">
   The grid
  estimation algorithm, a modified version of Dijkstra&rsquo;s shortest path
  </a>
  <span>
   , seeks to make connections in the most efficient way
  possible. Using known electrical grids as templates, grid paths are
  encouraged to follow roads, avoid water, and prefer shorter paths.
  </span>
 </p>
</div>
<div>
 <p>
  <a href="https://medium.com/airbnb-engineering/discovering-and-classifying-in-app-message-intent-at-airbnb-6a55f5400a0c">
   <span>
    (01/2019: AirBnB) Discovering and Classifying In-App
  Message Intent
   </span>
  </a>
 </p>
 <p>
  We
  needed an algorithm that can detect distinct underlying topics, and decide
  which one is the primary one based on probability scores. ~13% of our target
  messages have multi-intent.
 </p>
 <p>
  Why
  LDA instead of doc2vec or BERT?
 </p>
 <ul>
  <li>
   <span>
    LDA is a
       probabilistic model, which gives a probabilistic composition of topics
       in a message.
   </span>
  </li>
  <li>
   <span>
    LDA
       assumes each word is drawn from certain word distribution that
       characterizes a unique topic, and each message can contain many
       different topics. The word distribution allows human judgement to weigh
       in when deciding what each topic means.
   </span>
  </li>
 </ul>
 <p>
  First,
  we pilot labeled a small sample by having each message labeled by multiple
  people to evaluate labeling quality. We then refined the label definitions
  based off the inter-rater agreement for each intent label, and kicked off
  formal labeling with a much larger data size.
 </p>
 <p>
  We
  keep a small portion of messages that are labeled by multiple reviewers so
  that we could identify the limits in prediction accuracy that our model could
  achieve due to human-level error.
 </p>
 <p>
  Why
  CNN instead of RNN?
  <span>
   Implementation
  simplicity, reported high accuracy, and especially fast speed (at both
  training and inference time).
  </span>
 </p>
 <p>
  Certain
  preprocessing steps such as tagging certain information are especially
  helpful in reducing noise as they normalize information like URLs, emails,
  date, time, phone number, etc.
 </p>
 <p>
  <img alt="Intent Category 
Parking Situation 
Internet Availability and Access 
Check-in &amp; Check-out Time Negotiation 
Amenities Questions 
Precision 
86% 
74% 
79% 
Recall 
92% 
86% 
84% 
82% 
Table 2: Example categories that are well predicted 
Table 3 below shows some example categories that are not well predicted. 
Intent Category 
Recommendations for Generic Activities 
Recommendations for Specific Activities 
Precision 
400/6 
480/0 
Recall 
33% 
Table 3: Example categories that are not so well predicted " src="applied_ml_ai_files/FC7C9A0F-2C9F-C542-A571-0274A9AE60E9.png"/>
 </p>
 <p>
  There
  were two primary root causes for the misclassifications: human errors in
  labeling and label ambiguity.
 </p>
</div>
<div>
 <p>
  <a href="https://ai.googleblog.com/2018/12/top-shot-on-pixel-3.html">
   <span>
    (12/2018: Google AI) Top Shot on Pixel 3
   </span>
  </a>
 </p>
 <p>
  Top Shot captures
  up to 90 images from 1.5 seconds before and after the shutter press,
  selecting up to two alternative shots to save in high resolution
 </p>
 <p>
  Given Top Shot runs
  in the camera as a background process, it uses a hardware-accelerated
  MobileNet-based single shot detector (SSD) so that it can use very little
  power.
 </p>
 <p>
  Our neural network
  design detects low-level visual attributes in early layers, like whether the
  subject is blurry, and then dedicates additional compute and parameters
  toward more complex objective attributes like whether the subject's eyes are
  open, and subjective attributes like whether there is an emotional expression
  of amusement or surprise.
 </p>
 <p>
  To test it, we
  collected data from hundreds of volunteers, along with their opinions of
  which frames (out of up to 90) looked best. This donated dataset covers many
  typical use cases, e.g. portraits, selfies, actions, landscapes, etc.
 </p>
 <p>
  We evaluated the
  accuracy of each signal used in Top Shot on several different subgroups of
  people (based on gender, age, ethnicity, etc.), testing for accuracy of each
  signal across those subgroups.
 </p>
</div>
<div>
 <p>
  <a href="https://apenwarr.ca/log/20190201">
   <span>
    02/2019:
  (Apen Warr) Forget Privacy - You&rsquo;re Terrible at Targeting Anyway
   </span>
  </a>
 </p>
 <p>
  Everyone
  loves collecting data, but nobody loves analyzing it later.
 </p>
 <p>
  Dirty
  secret of the ML movement:
  <span>
   almost everything
  produced by ML could have been produced, more cheaply, using a very dumb
  heuristic you coded up by hand.
  </span>
 </p>
 <p>
  Examples
  of heuristics/algorithms that need no tracking and work better than 'ML'
  recommendations:
 </p>
 <ul>
  <li>
   <span>
    DuckDuckGo
       serving ads related to the thing that you were searching for
   </span>
  </li>
  <li>
   <span>
    Steam
       emailing me that the games on my wish list are on sale
   </span>
  </li>
  <li>
   <span>
    Pandora's
       hand-coded music genome project that recommends songs based on the
       starter, and a few thumb ups/downs
   </span>
  </li>
 </ul>
 <p>
  ML
  probably infers my gender, age, income level and marital status. After that
  it sells me cars if I'm a guy, and fashion if I'm a woman. Not because all
  guys like cars, but because some uncreative human said, "Please sell my
  car to mostly men" and "Please sell my fashion items mostly to
  women.
 </p>
 <p>
  There
  really are some excellent uses of ML out there, for things traditional
  algorithms are bad at, like image processing or winning at strategy games.
 </p>
</div>
<div>
 <p>
  <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/01/Traffic_Updates__Saying_a_Lot_While_Revealing_a_Little___10_pages___2019_AAAI_camera_ready.pdf">
   <span>
    01/2019: (Microsoft Research) Traffic Updates -
  Saying a lot while revealing little
   </span>
  </a>
 </p>
 <p>
  Problem:
  <span>
   Naively soliciting speed reports from all
  eligible vehicles reduces privacy and increases bandwidth requirements. Let's
  reduce the # of speed reports.
  </span>
 </p>
 <p>
  Technique:
  <span>
   Markov Random Field trained with historical
  speed data so that it can exploit probabilistic dependencies among traffic on
  different roads. Each vehicle gets fitted with one.
  </span>
 </p>
 <p>
  Key
  tenets:
 </p>
 <ul>
  <li>
   <span>
    If traffic is moving like it
       normally does (90% of the time), everyone can assume that everything is
       normal.
   </span>
  </li>
  <li>
   <span>
    If traffic is abnormal, only
       a few vehicles (8%) on a road segment need to report it. The model can
       infer the value of its speed report to the network.
   </span>
  </li>
  <li>
   <span>
    A speed report from one road
       can be used to infer speeds on other roads due to correlations between
       speeds on different roads.
   </span>
  </li>
 </ul>
</div>
<div>
 <p>
  <a href="https://www.microsoft.com/en-us/research/uploads/prod/2019/01/bios_bias.pdf">
   <span>
    01/2019: (Microsoft Research) Bias in Bios
   </span>
  </a>
 </p>
 <p>
  Used
  397,340 online biographies. Predicted the occupation stated in the first
  sentence using the rest of the biography using various semantic
  representations: bag-of-words, word embeddings and deep recurrent neural
  networks. Accuracy ranged from .78 to .85, with DNNs topping the charts.
 </p>
 <p>
  Similar
  accuracies obtained after scrubbing explicit gender indicators. The TPR
  gender gap reduced (w/ little tradeoff of performance for fairness). Still:
 </p>
 <ul>
  <li>
   <span>
    They were
       able to predict gender with .68 accuracy
   </span>
  </li>
  <li>
   <span>
    Some
       occupations have significant TPR gender gaps
   </span>
  </li>
  <li>
   <span>
    There's
       still +ve correlation between TPR gender gap for an occupation and the
       gender imbalance in that occupation.
   </span>
  </li>
  <ul>
   <li>
    <span>
     The Leaky Pipeline:
    </span>
    <span>
     if a
        classifier compounds existing gender imbalances, then the
        underrepresented gender will, over time, become even further
        underrepresented.
    </span>
   </li>
  </ul>
 </ul>
 <p>
  Top 5
  ML mistakes that the models rectify after gender indicators are swapped:
 </p>
 <ul>
  <li>
   <span>
    Men:
   </span>
   <span>
    attorney
       instead of paralegal, architect instead of interior designer, professor
       instead of dietitian, photographer instead of interior designer, teacher
       instead of yoga teacher.
   </span>
  </li>
  <li>
   <span>
    Women:
   </span>
   <span>
    rapper
       instead of model, pastor instead of teacher, SWE instead of professor,
       surgeon instead of professor, surgeon instead of physician.
   </span>
  </li>
 </ul>
</div>
<div>
 <p>
  <a href="https://www.microsoft.com/en-us/research/blog/creating-better-ai-partners-a-case-for-backward-compatibility/">
   <span>
    01/2019: (Microsoft Research) Creating Better AI
  Partners
   </span>
  </a>
 </p>
 <p>
  The
  success of human-AI teams in decision-making relies on the human partner to
  create a mental model and to learn when to trust (or override) the machine.
 </p>
 <p>
  Proposition:
  <span>
   augment the loss function with a (weighted)
  dissonance factor that measures divergence of models for data points where
  the old model was correct, penalizing new introduced errors. Price: loss in
  overall accuracy.
  </span>
 </p>
</div>
<div>
 <p>
  <span>
   ML is not, by default, fair
  </span>
 </p>
 <p>
  Training data can
  be biased, whether the bias features are explicit or implicit. And companies
  hide their data under 'trade secret'&hellip;
 </p>
 <p>
  Valid statistical
  patterns in large groups may not hold for minorities, e.g.
  <span>
   diverse names -&gt; fake profiles for white
  population, not so for other ethnic groups.
  </span>
 </p>
 <p>
  Humans tend to be
  biased in different ways, thus might cancel out. Algorithms do not.
 </p>
 <p>
  Even if subgroups
  admit simple classifiers, arbitrary combination of linear classifiers is
  computationally more expensive. Also, sometimes the law prohibits such
  segregation.
 </p>
 <p>
  <span>
   Source: SoECS meeting
  </span>
 </p>
</div>
<div>
 <p>
  <span>
   Minorities vs. ML
  </span>
 </p>
 <p>
  Minorities,
  by definition, have fewer data available. Increasing data size may further
  disadvantage minorities while still increasing model accuracy.
 </p>
 <p>
  <span>
   COMPAS algorithm for predicting crime propensity.
  Blacks are 2x as likely as whites to be labelled high risk. Accuracy rates
  for Blacks and Whites were both 60%.
  </span>
 </p>
 <p>
  Cornell
  Study:
  <span>
   A risk score could either be equally
  predictive or equally wrong for all races, but not both.
  </span>
  That is
  mathematics, not opinion.
 </p>
 <p>
  <span>
   Source: SoECS meeting
  </span>
 </p>
</div>
<div>
 <p>
  <a href="https://news.ycombinator.com/item?id=17392953">
   <span>
    06/2018 HN: (Ben Evans) Ways to think about ML
   </span>
  </a>
 </p>
 <p>
  People
  tend to humanize future tech. We did not get robotic servants. We got washing
  machines that don't understand clothes nor dishes.
 </p>
 <p>
  Think
  of ML as enabling technology layers. ML helps us find implicit and
  probabilistic patterns in data. Questions that were previously hard to
  explain to computers.
 </p>
 <p>
  ML
  gives you millions of interns that can do one discrete task at scale, e.g.
  <span>
   find all suspicious emails.
  </span>
  But some fields
  are narrow enough and deep enough,
  <span>
   e.g. Alpha
  Go,
  </span>
  that we can give ML a cost function. Looking at all of the data
  yields insights that no human ever could.
 </p>
 <p>
  Relational
  Databases allowed cross-referenced queries a routine thing to do,
  <span>
   e.g. find all customers that bought X and live in
  city Y.
  </span>
  Record-keeping systems were turned into business intelligence
  systems.
 </p>
 <p>
  Right
  now, everything has relational databases and no one cares. We might achieve
  the same with ML in a couple of years.
 </p>
 <p>
  Demos
  have voice and image recognition, but what would a normal company do with
  that?
 </p>
 <ul>
  <li>
   <span>
    Deliver better optimizations
       on current questions,
   </span>
   <span>
    e.g. Instacart routing personal shoppers
       through grocery stores at a 50% delivery improvement.
   </span>
  </li>
  <li>
   <span>
    Make new kinds of queries,
   </span>
   <span>
    e.g.
       lawyer searching for 'anxious' emails in addition to keyword searches.
   </span>
  </li>
  <li>
   <span>
    Opening up image and voice
       to analysis. Previously, computers only understood text and
       numbers.
   </span>
   &nbsp;
  </li>
 </ul>
</div>
<div>
 <p>
  <a href="https://news.ycombinator.com/item?id=17820626">
   <span>
    07/2018: Edge Computing at Chick-fil-A
   </span>
  </a>
 </p>
 <p>
  Restaurants
  usually use a spreadsheet with historical sales data to predict how much food
  to pre-cook by the hour. Overcook and waste, or undercook and make customers
  wait longer.
  Chick-Fil-A has centrally trained ML models that are deployed to Kubernetes
  clusters at the restaurants. They can now feed in live data for more accurate
  predictions at a low cost.
 </p>
</div>
