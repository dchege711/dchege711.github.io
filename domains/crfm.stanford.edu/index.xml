<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>crfm.stanford.edu on Chege's Blog</title><link>https://www.curiosities.dev/domains/crfm.stanford.edu/</link><description>Recent content in crfm.stanford.edu on Chege's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 30 Nov 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/domains/crfm.stanford.edu/index.xml" rel="self" type="application/rss+xml"/><item><title>Given Language Models, Why Learn About Large Language Models?</title><link>https://www.curiosities.dev/computer-science/large-language-models/why-learn-about-llms/</link><pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/why-learn-about-llms/</guid><description>This part of seems pertinent to respond to &amp;ldquo;LLMs are just (auto-complete; Markov chains; [insert pre-existing LM-adjacent tech]) on steroids&amp;rdquo;.
Scale LLMs are massive. From 2018 - 2022, model sizes have increased 5000x. OpenAI&amp;rsquo;s GPT model from June 2018 had 110M parameters; GPT-3 from May 2020 had 175B parameters. LLM providers no longer seem to advertise their parameter counts; GPT-4 was leaked to have 1.8T parameters.</description></item></channel></rss>