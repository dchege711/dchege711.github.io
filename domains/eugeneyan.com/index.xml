<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>eugeneyan.com on Chege's Blog</title><link>https://www.curiosities.dev/domains/eugeneyan.com/</link><description>Recent content in eugeneyan.com on Chege's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 06 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/domains/eugeneyan.com/index.xml" rel="self" type="application/rss+xml"/><item><title>LLM Evals</title><link>https://www.curiosities.dev/computer-science/large-language-models/llm-evals/</link><pubDate>Sun, 06 Apr 2025 00:00:00 +0000</pubDate><guid>https://www.curiosities.dev/computer-science/large-language-models/llm-evals/</guid><description>Notable Benchmarks Some notable benchmarks in language modeling:
MMLU: 57 tasks spanning elementary math, US history, computer science, law, and more. EleutherAI Eval: Unified framework to test models via zero/few-shot settings on 200 tasks from various evals, including MMLU. HELM: Evaluates LLMs across domains; tasks include Q&amp;amp;A, information retrieval, summarization, text classification, etc. AlpacaEval: Measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model.</description></item></channel></rss>