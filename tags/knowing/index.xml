<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Knowing on c13u&#39;s Blog</title>
    <link>https://www.curiosities.dev/tags/knowing/</link>
    <description>Recent content in Knowing on c13u&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 21 Mar 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/tags/knowing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Rationality: From AI to Zombies [Yudkowsky, Eliezer]</title>
      <link>https://www.curiosities.dev/knowing/yudkowsky-rationality/_meta/</link>
      <pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/yudkowsky-rationality/_meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>How to Study [Swain, George Fillmore]</title>
      <link>https://www.curiosities.dev/knowing/swain-how-to-study/_meta/</link>
      <pubDate>Mon, 01 Jan 1917 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/swain-how-to-study/_meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Factfulness [Hans Rosling; Anna Rosling RÃ¶nnlund; Ola Rosling]</title>
      <link>https://www.curiosities.dev/knowing/rosling-factfulness/_meta/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/rosling-factfulness/_meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Illustrated Book of Bad Arguments [Ali Almossawi]</title>
      <link>https://www.curiosities.dev/knowing/almossawi-bad-arguments/_meta/</link>
      <pubDate>Thu, 05 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/almossawi-bad-arguments/_meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Caveats on Similarity Learning</title>
      <link>https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/03-caveats-on-similarity-learning/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/computer-science/machine-learning/ele364-machine-learning-and-predictive-analytics/05-similarity-based-learning/03-caveats-on-similarity-learning/</guid>
      <description>Similarity-based learning is intuitive and gives people confidence in the model.
There is an inductive bias that instances that have similar descriptive features belong to the same class.
Remarkably so. When I think of classifying things, my mind immediately goes to NN.
  Similarity learning has a stationary assumption, i.e. the joint PDF of the data doesn&amp;rsquo;t change (new classifications do not come up). This assumption is shared by supervised ML.</description>
    </item>
    
    <item>
      <title>Caveats on Similarity Learning</title>
      <link>https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/caveats-on-similarity-learning/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/computer-science/machine-learning/similarity-based-learning/caveats-on-similarity-learning/</guid>
      <description>Similarity-based learning is intuitive and gives people confidence in the model.
There is an inductive bias that instances that have similar descriptive features belong to the same class.
Remarkably so. When I think of classifying things, my mind immediately goes to NN.
  Similarity learning has a stationary assumption, i.e. the joint PDF of the data doesn&amp;rsquo;t change (new classifications do not come up). This assumption is shared by supervised ML.</description>
    </item>
    
    <item>
      <title>Misconstructions and Misconceptions</title>
      <link>https://www.curiosities.dev/knowing/2021-08-02-misconstructions-and-misconceptions/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/2021-08-02-misconstructions-and-misconceptions/</guid>
      <description>A collection of instances in which I believed something that wasn&amp;rsquo;t true. A reminder to read not to believe, but to weigh and consider  .
  The Four Color Theorem does not claim that 4 colors suffice to color a planar map. Instead, 4 colors are sufficient to color any planar graph so that no two vertices connected by an edge are colored with the same color. For any \(n\), there is a map that requires at least \(n\) colors.</description>
    </item>
    
    <item>
      <title>Thoughts on Academic Research</title>
      <link>https://www.curiosities.dev/knowing/2021-06-15-on-academic-research/</link>
      <pubDate>Tue, 15 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/2021-06-15-on-academic-research/</guid>
      <description>How to Read a Paper  The First Pass (5 - 10 min) Objectives: category, context, validity of assumptions, contributions and quality of writing.
 Carefully read the title, abstract, and introduction. Read the section and sub-section headings, but ignore everything else Glance at the math to determine the underlying theoretical foundations Read the conclusions. Mentally tick off references that you&amp;rsquo;ve already read.  Most of the papers will not make it beyond this step.</description>
    </item>
    
    <item>
      <title>On Learning</title>
      <link>https://www.curiosities.dev/knowing/2020-05-02-on-learning/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/2020-05-02-on-learning/</guid>
      <description>Choose your content. Some material not worth the effort. Learning comes from repetition, but sometimes you just don&amp;rsquo;t care whether you&amp;rsquo;ll remember what you&amp;rsquo;ve read. 
But even if you don&amp;rsquo;t remember the specifics, the effect on your model of the world persists. As your mental model evolves, re-reading books is beneficial because the material compiles differently. 
When reading, annotate connections from previous knowledge, unanswered questions and unjustified assumptions. Make flashcards of facts/quotes that you wish to analyze.</description>
    </item>
    
    <item>
      <title>Knowing</title>
      <link>https://www.curiosities.dev/knowing/</link>
      <pubDate>Sat, 08 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/</guid>
      <description>This section deals with epistemology (theory of knowledge).</description>
    </item>
    
    <item>
      <title>[Summary] (Morgan Housel) Immeasurably Important</title>
      <link>https://www.curiosities.dev/personal_growth/an_outlook_on_life/2018-07-01-housel-immeasurably-important/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/personal_growth/an_outlook_on_life/2018-07-01-housel-immeasurably-important/</guid>
      <description>Immeasurably Important.  Morgan Housel. Collaborative Fund. http://www.collaborativefund.com/blog/immeasurably-important/  . Jul 5, 2018.  Filtering out information is an art, not a science, necessitated by the information overload that we live in.
 Watch out for the tendency to only preserve information that meshes with how we think the world should be. If you think the world is all art, you miss how much stuff is too complicated to think about intuitively.</description>
    </item>
    
    <item>
      <title>The Importance of Deep Work &amp; The 30-Hour Method for Learning a New Skill</title>
      <link>https://www.curiosities.dev/knowing/2018-05-26-azeria-labs-importance-of-deep-work/</link>
      <pubDate>Sat, 26 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/2018-05-26-azeria-labs-importance-of-deep-work/</guid>
      <description>The Importance of Deep Work &amp;amp; The 30-Hour Method for Learning a New Skill.  Azeria Labs. https://azeria-labs.com/the-importance-of-deep-work-the-30-hour-method-for-learning-a-new-skill/  . https://news.ycombinator.com/item?id=17163251  .  30 Hour Deep Work Tryout:
 Scan resources and pick the ones that seem most useful (5 hours) Set up the environment and define structured goals (3 hours) Work towards increasingly difficult goals set in step 2 (22 hours)  Each session should be about 4 hours.</description>
    </item>
    
    <item>
      <title>Brainless Slime That Can Learn by Fusing [The Atlantic]</title>
      <link>https://www.curiosities.dev/biology/2016-12-21-brainless-slime-that-can-learn-by-fusing/</link>
      <pubDate>Wed, 21 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/biology/2016-12-21-brainless-slime-that-can-learn-by-fusing/</guid>
      <description>Brainless Slime That Can Learn by Fusing.  The Atlantic. https://www.theatlantic.com/science/archive/2016/12/the-brainless-slime-that-can-learn-by-fusing/511295/  . https://old.reddit.com/r/todayilearned/comments/b245af/til_scientists_put_slime_mold_over_a_map_of_tokyo/  . Dec 21, 2016.  Building Transit Networks Can a cell learn? When a part of the plasmodium touches something attractive, e.g. food, it pulses more quickly and widens. If a part meets something repulsive, like light, it pulses more slowly and shrinks.
The article regards this as flowing in the best possible direction without conscious thought.</description>
    </item>
    
    <item>
      <title>What is Ergodicity?</title>
      <link>https://www.curiosities.dev/mathematics/probability/2016-11-23-syll-what-is-ergodicity/</link>
      <pubDate>Wed, 23 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/mathematics/probability/2016-11-23-syll-what-is-ergodicity/</guid>
      <description>A random process is ergodic if all of its statistics can be determined from a sample function of the process. That is, the ensemble averages equal the corresponding time averages with probability one.
Role of Ergodicity in Human Inference  A newspaper has previously printed some inaccurate information, therefore, the newspaper is going to publish inaccurate information in the future.  Fair. Ensemble of published articles is more or less ergodic.</description>
    </item>
    
    <item>
      <title>The Black Swan [Taleb, Nicholas Nassim]</title>
      <link>https://www.curiosities.dev/trading-and-investing/taleb-the-black-swan/_meta/</link>
      <pubDate>Tue, 17 Apr 2007 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/trading-and-investing/taleb-the-black-swan/_meta/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Toward a Theory of Medical Fallibility</title>
      <link>https://www.curiosities.dev/health-and-medicine/1976-01-01-toward-a-theory-of-medical-fallibility/</link>
      <pubDate>Thu, 01 Jan 1976 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/health-and-medicine/1976-01-01-toward-a-theory-of-medical-fallibility/</guid>
      <description>Medical care is like the opposite of moving fast and breaking things. If it&amp;rsquo;s so taboo to admit error, then that could make errors more common because fewer people are learning from past errors.
  Norms for Scientific Activity and the Sources of Error &amp;ldquo;Science&amp;rdquo; is taken to mean &amp;ldquo;Natural Science&amp;rdquo;.
  Internal norms derive from a cognitive pursuit of science. They are:
 Focus on the central rather than the peripheral problems of the science in in question.</description>
    </item>
    
  </channel>
</rss>
