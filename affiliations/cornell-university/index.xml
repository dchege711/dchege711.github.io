<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cornell University on Chege&#39;s Blog</title>
    <link>https://www.curiosities.dev/affiliations/Cornell-University/</link>
    <description>Recent content in Cornell University on Chege&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://www.curiosities.dev/affiliations/Cornell-University/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>On Bullshit [Frankfurt]</title>
      <link>https://www.curiosities.dev/knowing/calling-bullshit/01-intro-to-bullshit/02-frankfurt-on-bullshit/</link>
      <pubDate>Wed, 01 Jan 1986 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/knowing/calling-bullshit/01-intro-to-bullshit/02-frankfurt-on-bullshit/</guid>
      <description>Need to develop a theoretical understanding of bullshit. An account of what bullshit is and how it differs from what it is not.
This is a common way of defining concepts in relation to others. Also saw it in the definitions of algebra, geometry and analysis as mathematical categories    Remarks on Black&amp;rsquo;s &amp;ldquo;The Prevalence of Humbug&amp;rdquo;   Humbug: deceptive misrepresentation, short of lying, especially by pretentious word or deed, of somebody&amp;rsquo;s own thoughts, feelings or attitudes.</description>
    </item>
    
    <item>
      <title>Journal Reviews on Fairness</title>
      <link>https://www.curiosities.dev/computer-science/bias-and-fairness/2021-10-04-journal-reviews-on-fairness/</link>
      <pubDate>Mon, 04 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://www.curiosities.dev/computer-science/bias-and-fairness/2021-10-04-journal-reviews-on-fairness/</guid>
      <description>Meta ðŸ“‘    Instead of changing the data or learners in multiple ways and then see if fairness improves,  postulate that the root causes of bias are the prior decisions that generated the training data. These affect (a) what data was selected, and (b) the labels assigned to the examples. They propose the \(\text{Fair-SMOTE}\) (Fair Synthetic Minority Over Sampling Technique) algorithm which (1) removes biased labels (via situation testing: if the model&amp;rsquo;s prediction for a data point changes once all of the data points&#39; protected attributes  are flipped, then that label is biased and the data point is discarded), and (2) rebalances internal distributions such that based on a protected attribute, examples are equal in both positive and negative classes.</description>
    </item>
    
  </channel>
</rss>
